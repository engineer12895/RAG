{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9325397,"sourceType":"datasetVersion","datasetId":5649496}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qU langchain_community pypdf\n!pip install langchain-groq\n!pip install sentence_transformers\n!pip install faiss-gpu\n!pip install tiktoken\n!pip install chromadb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-06T09:21:43.063747Z","iopub.execute_input":"2024-09-06T09:21:43.064151Z","iopub.status.idle":"2024-09-06T09:23:30.675964Z","shell.execute_reply.started":"2024-09-06T09:21:43.064112Z","shell.execute_reply":"2024-09-06T09:23:30.674755Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.4 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.4 requires shapely<2.1,>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.9.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting langchain-groq\n  Downloading langchain_groq-0.1.9-py3-none-any.whl.metadata (2.9 kB)\nCollecting groq<1,>=0.4.1 (from langchain-groq)\n  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.26 in /opt/conda/lib/python3.10/site-packages (from langchain-groq) (0.2.38)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (2.8.2)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (6.0.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (0.1.115)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (24.1)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (8.3.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.7.4)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.4)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.10.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.32.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.20.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (1.26.18)\nDownloading langchain_groq-0.1.9-py3-none-any.whl (14 kB)\nDownloading groq-0.11.0-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: groq, langchain-groq\nSuccessfully installed groq-0.11.0 langchain-groq-0.1.9\nCollecting sentence_transformers\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.0)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.24.6)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.0.1\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nCollecting tiktoken\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\nDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.7.0\nCollecting chromadb\n  Downloading chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\nCollecting build>=1.0.3 (from chromadb)\n  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.8.2)\nCollecting chroma-hnswlib==0.7.6 (from chromadb)\n  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.111.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.1)\nRequirement already satisfied: numpy<2.0.0,>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.26.4)\nCollecting posthog>=2.4.0 (from chromadb)\n  Downloading posthog-3.6.3-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.12.2)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.25.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.25.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.25.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.19.1)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.66.4)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.4.0)\nRequirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.64.1)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.12.3)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (8.3.0)\nRequirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.0.2)\nCollecting mmh3>=4.0.1 (from chromadb)\n  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.10.4)\nRequirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.27.0)\nRequirement already satisfied: packaging>=19.1 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.1)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb)\n  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.0.4)\nRequirement already satisfied: jinja2>=2.11.2 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\nRequirement already satisfied: python-multipart>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.0.9)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (5.10.0)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (2.1.1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.7)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.30.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\nRequirement already satisfied: importlib-metadata<=7.1,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.0)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.1)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\nRequirement already satisfied: opentelemetry-proto==1.25.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\nCollecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (70.0.0)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation-asgi==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\nCollecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.20.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.24.6)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (13.7.1)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb) (2.6.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\nDownloading chromadb-0.5.5-py3-none-any.whl (584 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.1-py3-none-any.whl (21 kB)\nDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\nDownloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl (14 kB)\nDownloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\nDownloading posthog-3.6.3-py2.py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53726 sha256=a4cda5ad2da25d351803255277383bcb77779c7f0bf19592d62040b0f9e3b88a\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, mmh3, pyproject_hooks, opentelemetry-util-http, humanfriendly, chroma-hnswlib, bcrypt, backoff, asgiref, posthog, coloredlogs, build, opentelemetry-instrumentation, onnxruntime, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, chromadb\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 30.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 build-1.2.1 chroma-hnswlib-0.7.6 chromadb-0.5.5 coloredlogs-15.0.1 humanfriendly-10.0 kubernetes-30.1.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.19.2 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-asgi-0.46b0 opentelemetry-instrumentation-fastapi-0.46b0 opentelemetry-util-http-0.46b0 posthog-3.6.3 pypika-0.48.9 pyproject_hooks-1.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\nos.environ['LANGCHAIN_PROJECT'] = 'advanced-rag'\nos.environ['LANGCHAIN_API_KEY'] = \"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\"\nos.environ['GROQ_API_KEY'] = \"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\"","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:23:30.678719Z","iopub.execute_input":"2024-09-06T09:23:30.679184Z","iopub.status.idle":"2024-09-06T09:23:30.685055Z","shell.execute_reply.started":"2024-09-06T09:23:30.679132Z","shell.execute_reply":"2024-09-06T09:23:30.683918Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import bs4\nfrom langchain import hub\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_groq import ChatGroq\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.storage import InMemoryByteStore\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.documents import Document\nimport uuid\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:23:30.686462Z","iopub.execute_input":"2024-09-06T09:23:30.686870Z","iopub.status.idle":"2024-09-06T09:23:31.969079Z","shell.execute_reply.started":"2024-09-06T09:23:30.686825Z","shell.execute_reply":"2024-09-06T09:23:31.968153Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"loader = PyPDFLoader('/kaggle/input/ecoli-and-bacterias/AVONCHEM.pdf')\ndocs =loader.load()\n\nloader = PyPDFLoader('/kaggle/input/ecoli-and-bacterias/bacteria.pdf')\ndocs.extend(loader.load())","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:23:31.971072Z","iopub.execute_input":"2024-09-06T09:23:31.972590Z","iopub.status.idle":"2024-09-06T09:23:32.293124Z","shell.execute_reply.started":"2024-09-06T09:23:31.972551Z","shell.execute_reply":"2024-09-06T09:23:32.292087Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"docs","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:23:32.294247Z","iopub.execute_input":"2024-09-06T09:23:32.294579Z","iopub.status.idle":"2024-09-06T09:23:32.301989Z","shell.execute_reply.started":"2024-09-06T09:23:32.294545Z","shell.execute_reply":"2024-09-06T09:23:32.301030Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': '/kaggle/input/ecoli-and-bacterias/AVONCHEM.pdf', 'page': 0}, page_content='AVONCHEM  \\nMembrane Laury l Sulphate Broth ACM -1820 -O Pre -weighted size: 38.1g  \\n \\n1. What is M-Lauryl Sulfate Broth ? \\nM-Lauryl Sulfate Broth is a type of broth used to count Escherichia coli  and other \\ncoliform bacteria in water. It replaces Membrane Enriched Teepol Broth and is  \\nrecommended by the ISO Committee according to ISO 9308 -1: 1990.  \\n \\n2. Composition:  \\n \\nIngredients     Grams/Litre  \\n  Peptic digest of animal tissue   39.0 \\nYeast extract                6.0 \\nLactose     30.0 \\nSodium lauryl sulfate    1.0  \\nPhenol red     0.2  \\nFinal pH 7.4 +/ - 0.2 at 25°C  \\n \\n3. Storage Instruct ions: \\n\\uf0b7 Store prepared media below 8°C.  \\n\\uf0b7 Protected from direct light.  \\n\\uf0b7 Store dehydrated powder, in a dry place, in tightly -sealed containers at 2 -25°C.  \\n\\uf0b7 The appearance of the broth is red colored  and clear . \\n \\n4. Preparation : \\n\\uf0b7 Suspend 76.2 g of the broth powde r in 1000 ml of distilled water.  \\n\\uf0b7 Boil to dissolve completely.  \\n\\uf0b7 Dispense as needed and sterilize by steaming for 30 minutes on three consecutive \\ndays or by autoclaving at 121°C for 15 minutes.  \\n \\n5. Inoculation : \\n\\uf0b7 Filter water samples through a sterile me mbrane filter.  \\n\\uf0b7 Place the filter on a sterile absorbent pad saturated with the broth.  '),\n Document(metadata={'source': '/kaggle/input/ecoli-and-bacterias/AVONCHEM.pdf', 'page': 1}, page_content=' \\n \\n6. Incubation:  \\n\\uf0b7 Unchlorinated Waters : \\n\\uf0a7 Coliform organisms   \\nFirst, incubate for 4 hours at 30°C.  \\nThen, incubate for an additional 14 hours at 35°C.  \\n \\n\\uf0a7 Escherichia coli:  \\nFirst, incubate for 4 hours at 30°C.  \\nThen, incubate for an additional 14 hours at 44°C.  \\n \\n\\uf0b7 Chlorinated Waters:  \\nThese organisms benefit from an extended incubation:  \\nIncubate for 6 hours at 25°C.  \\nAfter incubation, yellow colonies may form, which should be further confirmed to ensure \\nthe presence of the organisms.  '),\n Document(metadata={'source': '/kaggle/input/ecoli-and-bacterias/bacteria.pdf', 'page': 0}, page_content='1. What is Salmonella Shigella (SS) Agar?  \\nSalmonella Shigella (SS) Agar is a selective medium used for isolating Salmonella  and Shigella  \\nspecies from clinical and environmental samples. It suppresses the growth of gram -positive \\nbacteria while promoting the growth of Salmonella . \\n2. Composition:  \\nIngredients  Grams/Litre  \\nPeptic digest of animal tissue  5.0 \\nBeef extract  5.0 \\nLactose  10.0 \\nSodiu m thiosulfate  8.5 \\nFerric citrate  1.0 \\nBile salts  8.5 \\nNeutral red  0.025  \\nSodium chloride  5.0 \\nAgar  15.0 \\nFinal pH  7.0 ± 0.2  \\n3. Storage Instructions:  \\n\\uf0b7 Store prepared media below 8°C.  \\n\\uf0b7 Keep protected from direct light.  \\n\\uf0b7 Dehydrated powder should be stored in a dry place, in tightly -sealed containers at 2 -\\n25°C.  \\n4. Preparation:  \\n\\uf0b7 Suspend 63 g of the dehydrated medium in 1000 ml  of distilled water.  \\n\\uf0b7 Boil to dissolve completely.  \\n\\uf0b7 Autoclave at 121°C for 15 minutes.  \\n5. Inoculation:  \\n\\uf0b7 Streak the sample directly onto the prepared medium using aseptic techniques.  \\n\\uf0b7 For environmental samples, filter through a sterile membrane and place on SS Agar.  \\n6. Incubation:  \\n\\uf0b7 Incubate the plates at 35°C for 18 -24 hours.  \\n\\uf0b7 Salmonella  colonies appear as colorless with black centers due to hydrogen sulfide \\nproduction.  ')]"},"metadata":{}}]},{"cell_type":"code","source":"chain = (\n    {\"doc\": lambda x: x.page_content}\n    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n    | ChatGroq()\n    | StrOutputParser()\n)\n\nsummaries = chain.batch(docs, {\"max_concurrency\": 1})","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:23:32.303245Z","iopub.execute_input":"2024-09-06T09:23:32.303587Z","iopub.status.idle":"2024-09-06T09:23:34.425287Z","shell.execute_reply.started":"2024-09-06T09:23:32.303545Z","shell.execute_reply":"2024-09-06T09:23:34.424472Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(summaries)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:23:34.426414Z","iopub.execute_input":"2024-09-06T09:23:34.426700Z","iopub.status.idle":"2024-09-06T09:23:34.432113Z","shell.execute_reply.started":"2024-09-06T09:23:34.426670Z","shell.execute_reply":"2024-09-06T09:23:34.430907Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['AVONCHEM Membrane Lauryl Sulfate Broth (ACM -1820 -O) is a broth used to enumerate Escherichia coli and other coliform bacteria in water, as an alternative to Membrane Enriched Teepol Broth and in line with ISO 9308 -1: 1990 recommendations. Its composition includes peptic digest of animal tissue (39.0 g/L), yeast extract (6.0 g/L), lactose (30.0 g/L), sodium lauryl sulfate (1.0 g/L), and phenol red (0.2 g/L), resulting in a final pH of 7.4 +/- 0.2 at 25°C. The powder should be stored in a dry place, while the prepared broth must be kept below 8°C, protected from light. Preparation involves suspending 76.2 g of the broth powder in 1000 ml of distilled water, boiling to dissolve, and sterilizing by steaming or autoclaving. Inoculation is carried out by filtering water samples through a sterile membrane and placing it on a sterile absorbent pad saturated with the broth.', 'The document outlines incubation procedures for testing water quality. For unchlorinated water, coliform organisms should be incubated for 4 hours at 30°C then for an additional 14 hours at 35°C. Escherichia coli should be incubated for 4 hours at 30°C then for an additional 14 hours at 44°C. For chlorinated water, both coliform and Escherichia coli organisms benefit from an extended incubation of 6 hours at 25°C. After incubation, yellow colonies may form which need to be confirmed for the presence of the organisms.', 'Salmonella Shigella (SS) Agar is a selective medium used for isolating Salmonella and Shigella species from clinical and environmental samples. It contains peptic digest of animal tissue, beef extract, lactose, sodium thiosulfate, ferric citrate, bile salts, neutral red, sodium chloride, and agar. The pH of the final solution is 7.0 ± 0.2. Prepared media should be stored below 8°C, protected from direct light, and dehydrated powder should be stored in a dry place, in tightly-sealed containers at 2 - 25°C. To prepare, suspend 63 g of the dehydrated medium in 1000 ml of distilled water, boil to dissolve completely, and autoclave at 121°C for 15 minutes. Inoculate by streaking the sample directly onto the prepared medium using aseptic techniques or filtering environmental samples through a sterile membrane and placing on SS Agar. Incubate the plates at 35°C for 18 -24 hours. Salmonella colonies will appear as colorless with black centers due to hydrogen sulfide production.']\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"BAAI/bge-small-en\"\nmodel_kwargs = {\"device\": \"cpu\"}\nencode_kwargs = {\"normalize_embeddings\": True}\nhf_embeddings = HuggingFaceBgeEmbeddings(\n    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:23:34.433583Z","iopub.execute_input":"2024-09-06T09:23:34.434234Z","iopub.status.idle":"2024-09-06T09:23:59.004529Z","shell.execute_reply.started":"2024-09-06T09:23:34.434188Z","shell.execute_reply":"2024-09-06T09:23:59.003627Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8d3735e87f84182b8717a9a23ad15a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a70b2bff4ca34d3bb8d2991e48279e3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/90.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0275b2da010d4013bc7b77921a11e050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"405712931fc94682b4a77a4b4a8680d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e178447be042469eb454aa1c74f6c602"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"319af68929be43bf8b70c3195906165d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fbdbf159cfe4fff98ee22c836522d07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3da6a6c9f86544818e66160dc7dbd6e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"880b256e4db24bd39e8a60aa41540c72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c13cb4b1b2ab4bdb9696d30069c5be12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b72041f63b8f4c7ba135a7ccbf777c95"}},"metadata":{}}]},{"cell_type":"code","source":"# The vectorstore to use to index the child chunks\nvectorstore = Chroma(collection_name=\"summaries\",\n                     embedding_function=hf_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:23:59.006030Z","iopub.execute_input":"2024-09-06T09:23:59.007251Z","iopub.status.idle":"2024-09-06T09:23:59.755800Z","shell.execute_reply.started":"2024-09-06T09:23:59.007203Z","shell.execute_reply":"2024-09-06T09:23:59.754759Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# The storage layer for the parent documents\nstore = InMemoryByteStore()\nid_key = \"doc_id\"","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:23:59.759753Z","iopub.execute_input":"2024-09-06T09:23:59.760467Z","iopub.status.idle":"2024-09-06T09:23:59.764605Z","shell.execute_reply.started":"2024-09-06T09:23:59.760428Z","shell.execute_reply":"2024-09-06T09:23:59.763490Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# The retriever\nretriever = MultiVectorRetriever(\n    vectorstore=vectorstore,\n    byte_store=store,\n    id_key=id_key,\n)\ndoc_ids = [str(uuid.uuid4()) for _ in docs]","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:23:59.766163Z","iopub.execute_input":"2024-09-06T09:23:59.766729Z","iopub.status.idle":"2024-09-06T09:23:59.780003Z","shell.execute_reply.started":"2024-09-06T09:23:59.766694Z","shell.execute_reply":"2024-09-06T09:23:59.779048Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Docs linked to summaries\nsummary_docs = [\n    Document(page_content=s, metadata={id_key: doc_ids[i]})\n    for i, s in enumerate(summaries)\n]","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:23:59.781174Z","iopub.execute_input":"2024-09-06T09:23:59.781522Z","iopub.status.idle":"2024-09-06T09:23:59.789630Z","shell.execute_reply.started":"2024-09-06T09:23:59.781487Z","shell.execute_reply":"2024-09-06T09:23:59.788718Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Add\nretriever.vectorstore.add_documents(summary_docs)\nretriever.docstore.mset(list(zip(doc_ids, docs)))","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:23:59.790774Z","iopub.execute_input":"2024-09-06T09:23:59.791078Z","iopub.status.idle":"2024-09-06T09:24:00.268094Z","shell.execute_reply.started":"2024-09-06T09:23:59.791047Z","shell.execute_reply":"2024-09-06T09:24:00.267029Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"doc_ids","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:24:00.269389Z","iopub.execute_input":"2024-09-06T09:24:00.269735Z","iopub.status.idle":"2024-09-06T09:24:00.276298Z","shell.execute_reply.started":"2024-09-06T09:24:00.269701Z","shell.execute_reply":"2024-09-06T09:24:00.275230Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['b016e1c8-6cab-4920-8232-018662349be5',\n '199ef161-c8b3-4864-bebb-3622d4bf63a6',\n 'e68ec337-96cd-4536-84e9-6e4b702eb814']"},"metadata":{}}]},{"cell_type":"code","source":"retriever.docstore.mget(doc_ids)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:24:00.277661Z","iopub.execute_input":"2024-09-06T09:24:00.278000Z","iopub.status.idle":"2024-09-06T09:24:00.287269Z","shell.execute_reply.started":"2024-09-06T09:24:00.277956Z","shell.execute_reply":"2024-09-06T09:24:00.286137Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': '/kaggle/input/ecoli-and-bacterias/AVONCHEM.pdf', 'page': 0}, page_content='AVONCHEM  \\nMembrane Laury l Sulphate Broth ACM -1820 -O Pre -weighted size: 38.1g  \\n \\n1. What is M-Lauryl Sulfate Broth ? \\nM-Lauryl Sulfate Broth is a type of broth used to count Escherichia coli  and other \\ncoliform bacteria in water. It replaces Membrane Enriched Teepol Broth and is  \\nrecommended by the ISO Committee according to ISO 9308 -1: 1990.  \\n \\n2. Composition:  \\n \\nIngredients     Grams/Litre  \\n  Peptic digest of animal tissue   39.0 \\nYeast extract                6.0 \\nLactose     30.0 \\nSodium lauryl sulfate    1.0  \\nPhenol red     0.2  \\nFinal pH 7.4 +/ - 0.2 at 25°C  \\n \\n3. Storage Instruct ions: \\n\\uf0b7 Store prepared media below 8°C.  \\n\\uf0b7 Protected from direct light.  \\n\\uf0b7 Store dehydrated powder, in a dry place, in tightly -sealed containers at 2 -25°C.  \\n\\uf0b7 The appearance of the broth is red colored  and clear . \\n \\n4. Preparation : \\n\\uf0b7 Suspend 76.2 g of the broth powde r in 1000 ml of distilled water.  \\n\\uf0b7 Boil to dissolve completely.  \\n\\uf0b7 Dispense as needed and sterilize by steaming for 30 minutes on three consecutive \\ndays or by autoclaving at 121°C for 15 minutes.  \\n \\n5. Inoculation : \\n\\uf0b7 Filter water samples through a sterile me mbrane filter.  \\n\\uf0b7 Place the filter on a sterile absorbent pad saturated with the broth.  '),\n Document(metadata={'source': '/kaggle/input/ecoli-and-bacterias/AVONCHEM.pdf', 'page': 1}, page_content=' \\n \\n6. Incubation:  \\n\\uf0b7 Unchlorinated Waters : \\n\\uf0a7 Coliform organisms   \\nFirst, incubate for 4 hours at 30°C.  \\nThen, incubate for an additional 14 hours at 35°C.  \\n \\n\\uf0a7 Escherichia coli:  \\nFirst, incubate for 4 hours at 30°C.  \\nThen, incubate for an additional 14 hours at 44°C.  \\n \\n\\uf0b7 Chlorinated Waters:  \\nThese organisms benefit from an extended incubation:  \\nIncubate for 6 hours at 25°C.  \\nAfter incubation, yellow colonies may form, which should be further confirmed to ensure \\nthe presence of the organisms.  '),\n Document(metadata={'source': '/kaggle/input/ecoli-and-bacterias/bacteria.pdf', 'page': 0}, page_content='1. What is Salmonella Shigella (SS) Agar?  \\nSalmonella Shigella (SS) Agar is a selective medium used for isolating Salmonella  and Shigella  \\nspecies from clinical and environmental samples. It suppresses the growth of gram -positive \\nbacteria while promoting the growth of Salmonella . \\n2. Composition:  \\nIngredients  Grams/Litre  \\nPeptic digest of animal tissue  5.0 \\nBeef extract  5.0 \\nLactose  10.0 \\nSodiu m thiosulfate  8.5 \\nFerric citrate  1.0 \\nBile salts  8.5 \\nNeutral red  0.025  \\nSodium chloride  5.0 \\nAgar  15.0 \\nFinal pH  7.0 ± 0.2  \\n3. Storage Instructions:  \\n\\uf0b7 Store prepared media below 8°C.  \\n\\uf0b7 Keep protected from direct light.  \\n\\uf0b7 Dehydrated powder should be stored in a dry place, in tightly -sealed containers at 2 -\\n25°C.  \\n4. Preparation:  \\n\\uf0b7 Suspend 63 g of the dehydrated medium in 1000 ml  of distilled water.  \\n\\uf0b7 Boil to dissolve completely.  \\n\\uf0b7 Autoclave at 121°C for 15 minutes.  \\n5. Inoculation:  \\n\\uf0b7 Streak the sample directly onto the prepared medium using aseptic techniques.  \\n\\uf0b7 For environmental samples, filter through a sterile membrane and place on SS Agar.  \\n6. Incubation:  \\n\\uf0b7 Incubate the plates at 35°C for 18 -24 hours.  \\n\\uf0b7 Salmonella  colonies appear as colorless with black centers due to hydrogen sulfide \\nproduction.  ')]"},"metadata":{}}]},{"cell_type":"code","source":"query = \"The heat for Escherichia coli \"\nsub_docs = vectorstore.similarity_search(query,k=1)\nsub_docs[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:24:00.288460Z","iopub.execute_input":"2024-09-06T09:24:00.288779Z","iopub.status.idle":"2024-09-06T09:24:00.328769Z","shell.execute_reply.started":"2024-09-06T09:24:00.288746Z","shell.execute_reply":"2024-09-06T09:24:00.327744Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Document(metadata={'doc_id': '199ef161-c8b3-4864-bebb-3622d4bf63a6'}, page_content='The document outlines incubation procedures for testing water quality. For unchlorinated water, coliform organisms should be incubated for 4 hours at 30°C then for an additional 14 hours at 35°C. Escherichia coli should be incubated for 4 hours at 30°C then for an additional 14 hours at 44°C. For chlorinated water, both coliform and Escherichia coli organisms benefit from an extended incubation of 6 hours at 25°C. After incubation, yellow colonies may form which need to be confirmed for the presence of the organisms.')"},"metadata":{}}]},{"cell_type":"code","source":"retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\nretrieved_docs[0].page_content[0:500]","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:24:00.329907Z","iopub.execute_input":"2024-09-06T09:24:00.330214Z","iopub.status.idle":"2024-09-06T09:24:00.414682Z","shell.execute_reply.started":"2024-09-06T09:24:00.330183Z","shell.execute_reply":"2024-09-06T09:24:00.413632Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"' \\n \\n6. Incubation:  \\n\\uf0b7 Unchlorinated Waters : \\n\\uf0a7 Coliform organisms   \\nFirst, incubate for 4 hours at 30°C.  \\nThen, incubate for an additional 14 hours at 35°C.  \\n \\n\\uf0a7 Escherichia coli:  \\nFirst, incubate for 4 hours at 30°C.  \\nThen, incubate for an additional 14 hours at 44°C.  \\n \\n\\uf0b7 Chlorinated Waters:  \\nThese organisms benefit from an extended incubation:  \\nIncubate for 6 hours at 25°C.  \\nAfter incubation, yellow colonies may form, which should be further confirmed to ensure \\nthe presence of the'"},"metadata":{}}]},{"cell_type":"markdown","source":"# ColBERT","metadata":{}},{"cell_type":"code","source":"! pip install ragatouille","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:26:14.055295Z","iopub.execute_input":"2024-09-06T09:26:14.055781Z","iopub.status.idle":"2024-09-06T09:26:41.817206Z","shell.execute_reply.started":"2024-09-06T09:26:14.055735Z","shell.execute_reply":"2024-09-06T09:26:41.815911Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting ragatouille\n  Downloading ragatouille-0.0.8.post4-py3-none-any.whl.metadata (15 kB)\nCollecting colbert-ai==0.2.19 (from ragatouille)\n  Downloading colbert-ai-0.2.19.tar.gz (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting faiss-cpu<2.0.0,>=1.7.4 (from ragatouille)\n  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nCollecting fast-pytorch-kmeans==0.2.0.1 (from ragatouille)\n  Downloading fast_pytorch_kmeans-0.2.0.1-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: langchain>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from ragatouille) (0.2.16)\nRequirement already satisfied: langchain_core>=0.1.4 in /opt/conda/lib/python3.10/site-packages (from ragatouille) (0.2.38)\nCollecting llama-index>=0.7 (from ragatouille)\n  Downloading llama_index-0.11.6-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: onnx<2.0.0,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from ragatouille) (1.16.2)\nCollecting sentence-transformers<3.0.0,>=2.2.2 (from ragatouille)\n  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: srsly==2.4.8 in /opt/conda/lib/python3.10/site-packages (from ragatouille) (2.4.8)\nRequirement already satisfied: torch>=1.13 in /opt/conda/lib/python3.10/site-packages (from ragatouille) (2.4.0+cpu)\nRequirement already satisfied: transformers<5.0.0,>=4.36.2 in /opt/conda/lib/python3.10/site-packages (from ragatouille) (4.44.0)\nCollecting voyager<3.0.0,>=2.0.2 (from ragatouille)\n  Downloading voyager-2.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\nCollecting bitarray (from colbert-ai==0.2.19->ragatouille)\n  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (2.21.0)\nRequirement already satisfied: flask in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (3.0.3)\nCollecting git-python (from colbert-ai==0.2.19->ragatouille)\n  Downloading git_python-1.0.3-py2.py3-none-any.whl.metadata (331 bytes)\nRequirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (1.0.1)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (1.11.1.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (1.14.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (4.66.4)\nRequirement already satisfied: ujson in /opt/conda/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (5.10.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (1.26.4)\nCollecting pynvml (from fast-pytorch-kmeans==0.2.0.1->ragatouille)\n  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.3 in /opt/conda/lib/python3.10/site-packages (from srsly==2.4.8->ragatouille) (2.0.10)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from faiss-cpu<2.0.0,>=1.7.4->ragatouille) (24.1)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.0->ragatouille) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.0->ragatouille) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.0->ragatouille) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.0->ragatouille) (4.0.3)\nRequirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.0->ragatouille) (0.2.4)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.0->ragatouille) (0.1.115)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.0->ragatouille) (2.8.2)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.0->ragatouille) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.0->ragatouille) (8.3.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain_core>=0.1.4->ragatouille) (1.33)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain_core>=0.1.4->ragatouille) (4.12.2)\nCollecting llama-index-agent-openai<0.4.0,>=0.3.0 (from llama-index>=0.7->ragatouille)\n  Downloading llama_index_agent_openai-0.3.0-py3-none-any.whl.metadata (728 bytes)\nCollecting llama-index-cli<0.4.0,>=0.3.0 (from llama-index>=0.7->ragatouille)\n  Downloading llama_index_cli-0.3.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting llama-index-core<0.12.0,>=0.11.6 (from llama-index>=0.7->ragatouille)\n  Downloading llama_index_core-0.11.6-py3-none-any.whl.metadata (2.4 kB)\nCollecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index>=0.7->ragatouille)\n  Downloading llama_index_embeddings_openai-0.2.4-py3-none-any.whl.metadata (635 bytes)\nCollecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index>=0.7->ragatouille)\n  Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index>=0.7->ragatouille)\n  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\nCollecting llama-index-llms-openai<0.3.0,>=0.2.2 (from llama-index>=0.7->ragatouille)\n  Downloading llama_index_llms_openai-0.2.2-py3-none-any.whl.metadata (705 bytes)\nCollecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index>=0.7->ragatouille)\n  Downloading llama_index_multi_modal_llms_openai-0.2.0-py3-none-any.whl.metadata (728 bytes)\nCollecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index>=0.7->ragatouille)\n  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\nCollecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index>=0.7->ragatouille)\n  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\nCollecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index>=0.7->ragatouille)\n  Downloading llama_index_readers_file-0.2.1-py3-none-any.whl.metadata (5.4 kB)\nCollecting llama-index-readers-llama-parse>=0.3.0 (from llama-index>=0.7->ragatouille)\n  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\nCollecting nltk>3.8.1 (from llama-index>=0.7->ragatouille)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (3.20.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.2.2)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.24.6)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->ragatouille) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->ragatouille) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->ragatouille) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->ragatouille) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->ragatouille) (2024.6.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.19.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (1.9.4)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain_core>=0.1.4->ragatouille) (2.4)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain>=0.1.0->ragatouille) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain>=0.1.0->ragatouille) (3.10.4)\nCollecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.0->llama-index>=0.7->ragatouille)\n  Downloading openai-1.43.1-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.6->llama-index>=0.7->ragatouille) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.6->llama-index>=0.7->ragatouille) (1.2.14)\nCollecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.6->llama-index>=0.7->ragatouille)\n  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.6->llama-index>=0.7->ragatouille) (1.6.0)\nRequirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.6->llama-index>=0.7->ragatouille) (0.7.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.6->llama-index>=0.7->ragatouille) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.6->llama-index>=0.7->ragatouille) (1.16.0)\nCollecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index>=0.7->ragatouille)\n  Downloading llama_cloud-0.0.17-py3-none-any.whl.metadata (751 bytes)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index>=0.7->ragatouille) (2.2.2)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index>=0.7->ragatouille) (4.12.3)\nRequirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index>=0.7->ragatouille) (4.3.1)\nCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index>=0.7->ragatouille)\n  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index>=0.7->ragatouille)\n  Downloading llama_parse-0.5.2-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index>=0.7->ragatouille) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index>=0.7->ragatouille) (1.4.2)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain>=0.1.0->ragatouille) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain>=0.1.0->ragatouille) (2.20.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.1.0->ragatouille) (3.0.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.3.8)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.70.16)\nRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (3.0.3)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.2.0)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (1.8.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13->ragatouille) (2.1.5)\nRequirement already satisfied: gitpython in /opt/conda/lib/python3.10/site-packages (from git-python->colbert-ai==0.2.19->ragatouille) (3.1.43)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.5.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13->ragatouille) (1.3.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index>=0.7->ragatouille) (2.5)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.1.0->ragatouille) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.1.0->ragatouille) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.1.0->ragatouille) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.1.0->ragatouille) (0.14.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index>=0.7->ragatouille) (1.9.0)\nCollecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index>=0.7->ragatouille)\n  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.6->llama-index>=0.7->ragatouille) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.6->llama-index>=0.7->ragatouille) (3.21.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython->git-python->colbert-ai==0.2.19->ragatouille) (4.0.11)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index>=0.7->ragatouille) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index>=0.7->ragatouille) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index>=0.7->ragatouille) (2024.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.1.0->ragatouille) (1.2.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille) (5.0.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index>=0.7->ragatouille) (1.16.0)\nDownloading ragatouille-0.0.8.post4-py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fast_pytorch_kmeans-0.2.0.1-py3-none-any.whl (8.8 kB)\nDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading llama_index-0.11.6-py3-none-any.whl (6.8 kB)\nDownloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading voyager-2.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading llama_index_agent_openai-0.3.0-py3-none-any.whl (13 kB)\nDownloading llama_index_cli-0.3.0-py3-none-any.whl (27 kB)\nDownloading llama_index_core-0.11.6-py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_index_embeddings_openai-0.2.4-py3-none-any.whl (6.1 kB)\nDownloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl (9.5 kB)\nDownloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_index_llms_openai-0.2.2-py3-none-any.whl (12 kB)\nDownloading llama_index_multi_modal_llms_openai-0.2.0-py3-none-any.whl (5.9 kB)\nDownloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\nDownloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\nDownloading llama_index_readers_file-0.2.1-py3-none-any.whl (38 kB)\nDownloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\nDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\nDownloading llama_cloud-0.0.17-py3-none-any.whl (187 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.4/187.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_parse-0.5.2-py3-none-any.whl (9.5 kB)\nDownloading openai-1.43.1-py3-none-any.whl (365 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\nDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: colbert-ai\n  Building wheel for colbert-ai (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for colbert-ai: filename=colbert_ai-0.2.19-py3-none-any.whl size=114762 sha256=01c5ea67a09c0302b6e462c08ef600dd89a49ca248114a363549248b12eb8117\n  Stored in directory: /root/.cache/pip/wheels/90/b9/63/d4fc276c73c42ef7fc1065a26cf87e5a1cf56ef6498cbfbe5d\nSuccessfully built colbert-ai\nInstalling collected packages: striprtf, dirtyjson, bitarray, voyager, pynvml, nltk, jiter, faiss-cpu, openai, llama-index-core, llama-cloud, git-python, fast-pytorch-kmeans, llama-parse, llama-index-readers-file, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, sentence-transformers, llama-index-readers-llama-parse, colbert-ai, llama-index-llms-openai, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index, ragatouille\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: sentence-transformers\n    Found existing installation: sentence-transformers 3.0.1\n    Uninstalling sentence-transformers-3.0.1:\n      Successfully uninstalled sentence-transformers-3.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitarray-2.9.2 colbert-ai-0.2.19 dirtyjson-1.0.8 faiss-cpu-1.8.0.post1 fast-pytorch-kmeans-0.2.0.1 git-python-1.0.3 jiter-0.5.0 llama-cloud-0.0.17 llama-index-0.11.6 llama-index-agent-openai-0.3.0 llama-index-cli-0.3.0 llama-index-core-0.11.6 llama-index-embeddings-openai-0.2.4 llama-index-indices-managed-llama-cloud-0.3.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.2 llama-index-multi-modal-llms-openai-0.2.0 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.1 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.2 nltk-3.9.1 openai-1.43.1 pynvml-11.5.3 ragatouille-0.0.8.post4 sentence-transformers-2.7.0 striprtf-0.0.26 voyager-2.0.9\n","output_type":"stream"}]},{"cell_type":"code","source":"from ragatouille import RAGPretrainedModel\nRAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:26:55.327969Z","iopub.execute_input":"2024-09-06T09:26:55.328929Z","iopub.status.idle":"2024-09-06T09:27:33.190597Z","shell.execute_reply.started":"2024-09-06T09:26:55.328882Z","shell.execute_reply":"2024-09-06T09:27:33.189671Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bd25c0f68174b2d90e6cec6e13fab9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b8fbf2fde8342a685431bff9f1cc7b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e49bdce48d24d849dab9f3dc96c4fd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"848db55ae60a43f3a8ae88e67547b361"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27497e9bfc6141d39c7c9b481e7e63f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d058ea5c44204707a323c9564acdbaa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1832fe70a62745efa4e47ce95ffb5325"}},"metadata":{}},{"name":"stdout","text":"[Sep 06, 09:27:02] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\n\ndef get_wikipedia_page(title: str):\n    \"\"\"\n    Retrieve the full text content of a Wikipedia page.\n\n    :param title: str - Title of the Wikipedia page.\n    :return: str - Full text content of the page as raw string.\n    \"\"\"\n    # Wikipedia API endpoint\n    URL = \"https://en.wikipedia.org/w/api.php\"\n\n    # Parameters for the API request\n    params = {\n        \"action\": \"query\",\n        \"format\": \"json\",\n        \"titles\": title,\n        \"prop\": \"extracts\",\n        \"explaintext\": True,\n    }\n\n    # Custom User-Agent header to comply with Wikipedia's best practices\n    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n\n    response = requests.get(URL, params=params, headers=headers)\n    data = response.json()\n\n    # Extracting page content\n    page = next(iter(data[\"query\"][\"pages\"].values()))\n    return page[\"extract\"] if \"extract\" in page else None\n\nfull_document = get_wikipedia_page(\"Document_retrieval\")","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:30:59.352491Z","iopub.execute_input":"2024-09-06T09:30:59.353593Z","iopub.status.idle":"2024-09-06T09:30:59.531596Z","shell.execute_reply.started":"2024-09-06T09:30:59.353546Z","shell.execute_reply":"2024-09-06T09:30:59.530677Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"full_document","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:31:39.794837Z","iopub.execute_input":"2024-09-06T09:31:39.795278Z","iopub.status.idle":"2024-09-06T09:31:39.802516Z","shell.execute_reply.started":"2024-09-06T09:31:39.795239Z","shell.execute_reply":"2024-09-06T09:31:39.801396Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'Document retrieval is defined as the matching of some stated user query against a set of free-text records. These records could be any type of mainly unstructured text, such as newspaper articles, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.\\nDocument retrieval is sometimes referred to as, or as a branch of, text retrieval. Text retrieval is a branch of information retrieval where the information is stored primarily in the form of text. Text databases became decentralized thanks to the personal computer. Text retrieval is a critical area of study today, since it is the fundamental basis of all internet search engines.\\n\\n\\n== Description ==\\nDocument retrieval systems find information to given criteria by matching text records (documents) against user queries, as opposed to expert systems that answer questions by inferring over a logical knowledge database. A document retrieval system consists of a database of documents, a classification algorithm to build a full text index, and a user interface to access the database.\\nA document retrieval system has two main tasks:\\n\\nFind relevant documents to user queries\\nEvaluate the matching results and sort them according to relevance, using algorithms such as PageRank.\\nInternet search engines are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using statistical or natural language processing techniques.\\n\\n\\n== Variations ==\\nThere are two main classes of indexing schemata for document retrieval systems: form based (or word based), and content based indexing. The document classification scheme (or indexing algorithm) in use determines the nature of the document retrieval system.\\n\\n\\n=== Form based ===\\nForm based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A suffix tree algorithm is an example for form based indexing.\\n\\n\\n=== Content based ===\\nThe content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an inverted index algorithm.\\nA signature file is a technique that creates a quick and dirty filter, for example a Bloom filter, that will keep all the documents that match to the query and hopefully a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to inverted files in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.\\n\\n\\n== Example: PubMed ==\\nThe PubMed form interface features the \"related articles\" search which works through a comparison of words from the documents\\' title, abstract, and MeSH terms using a word-weighted algorithm.\\n\\n\\n== See also ==\\nCompound term processing\\nDocument classification\\nEnterprise search\\nEvaluation measures (information retrieval)\\nFull text search\\nInformation retrieval\\nLatent semantic indexing\\nSearch engine\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nFaloutsos, Christos; Christodoulakis, Stavros (1984). \"Signature files: An access method for documents and its analytical performance evaluation\". ACM Transactions on Information Systems. 2 (4): 267–288. doi:10.1145/2275.357411. S2CID 8120705.\\nJustin Zobel; Alistair Moffat; Kotagiri Ramamohanarao (1998). \"Inverted files versus signature files for text indexing\" (PDF). ACM Transactions on Database Systems. 23 (4): 453–490. CiteSeerX 10.1.1.54.8753. doi:10.1145/296854.277632. S2CID 7293918.\\nBen Carterette; Fazli Can (2005). \"Comparing inverted files and signature files for searching a large lexicon\" (PDF). Information Processing and Management. 41 (3): 613–633. doi:10.1016/j.ipm.2003.12.003.\\n\\n\\n== External links ==\\n\\nFormal Foundation of Information Retrieval, Buckinghamshire Chilterns University College'"},"metadata":{}}]},{"cell_type":"code","source":"use_faiss=True\nRAG.index(\n    collection=[full_document],\n    index_name=\"Doc-1\",use_faiss=True,\n    max_document_length=180,\n    split_documents=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:39:28.288527Z","iopub.execute_input":"2024-09-06T09:39:28.289552Z","iopub.status.idle":"2024-09-06T09:39:52.839062Z","shell.execute_reply.started":"2024-09-06T09:39:28.289509Z","shell.execute_reply":"2024-09-06T09:39:52.837917Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"New index_name received! Updating current index_name (Doc-1) to Doc-1\n\n\n[Sep 06, 09:39:28] #> Note: Output directory .ragatouille/colbert/indexes/Doc-1 already exists\n\n\n[Sep 06, 09:39:28] #> Will delete 10 files already at .ragatouille/colbert/indexes/Doc-1 in 20 seconds...\n[Sep 06, 09:39:49] [0] \t\t #> Encoding 7 passages..\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:01<00:00,  1.57s/it]","output_type":"stream"},{"name":"stdout","text":"[Sep 06, 09:39:51] [0] \t\t avg_doclen_est = 116.42857360839844 \t len(local_sample) = 7\n[Sep 06, 09:39:51] [0] \t\t Creating 256 partitions.\n[Sep 06, 09:39:51] [0] \t\t *Estimated* 815 embeddings.\n[Sep 06, 09:39:51] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/Doc-1/plan.json ..\n","output_type":"stream"},{"name":"stderr","text":"\nWARNING clustering 775 points to 256 centroids: please provide at least 9984 training points\n","output_type":"stream"},{"name":"stdout","text":"Clustering 775 points in 128D to 256 clusters, redo 1 times, 20 iterations\n  Preprocessing in 0.00 s\n[0.04, 0.033, 0.036, 0.033, 0.034, 0.031, 0.025, 0.032, 0.028, 0.027, 0.031, 0.024, 0.024, 0.031, 0.035, 0.034, 0.023, 0.027, 0.027, 0.031, 0.032, 0.031, 0.032, 0.031, 0.034, 0.031, 0.033, 0.032, 0.037, 0.033, 0.036, 0.036, 0.037, 0.041, 0.03, 0.029, 0.031, 0.034, 0.027, 0.037, 0.033, 0.031, 0.034, 0.032, 0.035, 0.024, 0.031, 0.033, 0.027, 0.031, 0.03, 0.039, 0.042, 0.031, 0.033, 0.048, 0.036, 0.04, 0.04, 0.032, 0.033, 0.045, 0.036, 0.04, 0.033, 0.043, 0.033, 0.033, 0.027, 0.031, 0.036, 0.024, 0.044, 0.031, 0.037, 0.027, 0.034, 0.035, 0.035, 0.036, 0.038, 0.039, 0.028, 0.039, 0.032, 0.029, 0.032, 0.033, 0.031, 0.036, 0.025, 0.027, 0.033, 0.036, 0.033, 0.022, 0.031, 0.032, 0.037, 0.026, 0.031, 0.035, 0.033, 0.03, 0.036, 0.028, 0.023, 0.021, 0.035, 0.027, 0.038, 0.033, 0.028, 0.035, 0.043, 0.037, 0.035, 0.034, 0.03, 0.03, 0.032, 0.035, 0.028, 0.038, 0.026, 0.034, 0.031, 0.03]\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[Sep 06, 09:39:51] [0] \t\t #> Encoding 7 passages..\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\u001b[A\n1it [00:01,  1.62s/it]\n100%|██████████| 1/1 [00:00<00:00, 1502.80it/s]","output_type":"stream"},{"name":"stdout","text":"[Sep 06, 09:39:52] #> Optimizing IVF to store map from centroids to list of pids..\n[Sep 06, 09:39:52] #> Building the emb2pid mapping..\n[Sep 06, 09:39:52] len(emb2pid) = 815\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 256/256 [00:00<00:00, 49816.36it/s]","output_type":"stream"},{"name":"stdout","text":"[Sep 06, 09:39:52] #> Saved optimized IVF to .ragatouille/colbert/indexes/Doc-1/ivf.pid.pt\nDone indexing!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'.ragatouille/colbert/indexes/Doc-1'"},"metadata":{}}]},{"cell_type":"code","source":"results = RAG.search(query=\"What is an example for form based indexing?\", k=3)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:42:20.740442Z","iopub.execute_input":"2024-09-06T09:42:20.740872Z","iopub.status.idle":"2024-09-06T09:42:20.831778Z","shell.execute_reply.started":"2024-09-06T09:42:20.740833Z","shell.execute_reply":"2024-09-06T09:42:20.830854Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:42:24.105593Z","iopub.execute_input":"2024-09-06T09:42:24.106001Z","iopub.status.idle":"2024-09-06T09:42:24.113160Z","shell.execute_reply.started":"2024-09-06T09:42:24.105964Z","shell.execute_reply":"2024-09-06T09:42:24.112028Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[{'content': '== Variations ==\\nThere are two main classes of indexing schemata for document retrieval systems: form based (or word based), and content based indexing. The document classification scheme (or indexing algorithm) in use determines the nature of the document retrieval system.\\n\\n\\n=== Form based ===\\nForm based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A suffix tree algorithm is an example for form based indexing.',\n  'score': 25.981781005859375,\n  'rank': 1,\n  'document_id': '791111a2-2420-45db-8254-163de09a81ab',\n  'passage_id': 2},\n {'content': '== Example: PubMed ==\\nThe PubMed form interface features the \"related articles\" search which works through a comparison of words from the documents\\' title, abstract, and MeSH terms using a word-weighted algorithm.\\n\\n\\n== See also ==\\nCompound term processing\\nDocument classification\\nEnterprise search\\nEvaluation measures (information retrieval)\\nFull text search\\nInformation retrieval\\nLatent semantic indexing\\nSearch engine\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nFaloutsos, Christos; Christodoulakis, Stavros (1984). \"Signature files: An access method for documents and its analytical performance evaluation\". ACM Transactions on Information Systems. 2 (4): 267–288. doi:10.1145/2275.357411. S2CID 8120705.',\n  'score': 19.317100524902344,\n  'rank': 2,\n  'document_id': '791111a2-2420-45db-8254-163de09a81ab',\n  'passage_id': 4},\n {'content': '=== Content based ===\\nThe content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an inverted index algorithm.\\nA signature file is a technique that creates a quick and dirty filter, for example a Bloom filter, that will keep all the documents that match to the query and hopefully a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to inverted files in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.',\n  'score': 18.938833236694336,\n  'rank': 3,\n  'document_id': '791111a2-2420-45db-8254-163de09a81ab',\n  'passage_id': 3}]"},"metadata":{}}]},{"cell_type":"code","source":"retriever = RAG.as_langchain_retriever(k=3)\nretriever.invoke(\"What is an example for form based indexing?\")","metadata":{"execution":{"iopub.status.busy":"2024-09-06T09:43:17.533112Z","iopub.execute_input":"2024-09-06T09:43:17.534004Z","iopub.status.idle":"2024-09-06T09:43:17.644081Z","shell.execute_reply.started":"2024-09-06T09:43:17.533954Z","shell.execute_reply":"2024-09-06T09:43:17.643030Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"[Document(page_content='== Variations ==\\nThere are two main classes of indexing schemata for document retrieval systems: form based (or word based), and content based indexing. The document classification scheme (or indexing algorithm) in use determines the nature of the document retrieval system.\\n\\n\\n=== Form based ===\\nForm based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A suffix tree algorithm is an example for form based indexing.'),\n Document(page_content='== Example: PubMed ==\\nThe PubMed form interface features the \"related articles\" search which works through a comparison of words from the documents\\' title, abstract, and MeSH terms using a word-weighted algorithm.\\n\\n\\n== See also ==\\nCompound term processing\\nDocument classification\\nEnterprise search\\nEvaluation measures (information retrieval)\\nFull text search\\nInformation retrieval\\nLatent semantic indexing\\nSearch engine\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nFaloutsos, Christos; Christodoulakis, Stavros (1984). \"Signature files: An access method for documents and its analytical performance evaluation\". ACM Transactions on Information Systems. 2 (4): 267–288. doi:10.1145/2275.357411. S2CID 8120705.'),\n Document(page_content='=== Content based ===\\nThe content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an inverted index algorithm.\\nA signature file is a technique that creates a quick and dirty filter, for example a Bloom filter, that will keep all the documents that match to the query and hopefully a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to inverted files in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.')]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}