{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9279380,"sourceType":"datasetVersion","datasetId":5616354}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Now first install important libraries","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain_community pypdf\n!pip install langchain-groq\n!pip install sentence_transformers\n!pip install faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:42:26.089130Z","iopub.execute_input":"2024-09-07T05:42:26.089977Z","iopub.status.idle":"2024-09-07T05:43:26.798070Z","shell.execute_reply.started":"2024-09-07T05:42:26.089935Z","shell.execute_reply":"2024-09-07T05:43:26.796956Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.2 requires cubinlinker, which is not installed.\ncudf 24.8.2 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.2 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.2 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.8.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.4 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.4 requires shapely<2.1,>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.8.1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.9.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting langchain-groq\n  Downloading langchain_groq-0.1.9-py3-none-any.whl.metadata (2.9 kB)\nCollecting groq<1,>=0.4.1 (from langchain-groq)\n  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.26 in /opt/conda/lib/python3.10/site-packages (from langchain-groq) (0.2.38)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (2.8.2)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (6.0.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (1.33)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (0.1.116)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (24.1)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (8.3.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.7.4)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.4)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.10.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.32.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.20.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (1.26.18)\nDownloading langchain_groq-0.1.9-py3-none-any.whl (14 kB)\nDownloading groq-0.11.0-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: groq, langchain-groq\nSuccessfully installed groq-0.11.0 langchain-groq-0.1.9\nCollecting sentence_transformers\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.0)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.24.6)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.0.1\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Now put your API keys","metadata":{}},{"cell_type":"code","source":"import os\n#os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n#os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\nos.environ['LANGCHAIN_PROJECT'] = 'advanced-rag'\nos.environ['LANGCHAIN_API_KEY'] = \"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\"\nos.environ['GROQ_API_KEY'] = \"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\"","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:43:26.800161Z","iopub.execute_input":"2024-09-07T05:43:26.800515Z","iopub.status.idle":"2024-09-07T05:43:26.805972Z","shell.execute_reply.started":"2024-09-07T05:43:26.800481Z","shell.execute_reply":"2024-09-07T05:43:26.805144Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# import the important dependencies","metadata":{}},{"cell_type":"code","source":"import bs4\nfrom langchain import hub\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_groq import ChatGroq\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.load import dumps, loads","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:43:26.807275Z","iopub.execute_input":"2024-09-07T05:43:26.807589Z","iopub.status.idle":"2024-09-07T05:43:28.069561Z","shell.execute_reply.started":"2024-09-07T05:43:26.807558Z","shell.execute_reply":"2024-09-07T05:43:28.068784Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## load pdf documents","metadata":{}},{"cell_type":"code","source":"loader = PyPDFLoader(\"/kaggle/input/thesis/Thesis.pdf\",)\ndocs = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:43:28.072079Z","iopub.execute_input":"2024-09-07T05:43:28.072985Z","iopub.status.idle":"2024-09-07T05:43:30.184077Z","shell.execute_reply.started":"2024-09-07T05:43:28.072940Z","shell.execute_reply":"2024-09-07T05:43:30.183022Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"docs","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:43:30.185350Z","iopub.execute_input":"2024-09-07T05:43:30.185683Z","iopub.status.idle":"2024-09-07T05:43:30.208736Z","shell.execute_reply.started":"2024-09-07T05:43:30.185651Z","shell.execute_reply":"2024-09-07T05:43:30.207809Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 0}, page_content='1 \\n             Loan -Approval -Prediction using machine learning  \\n                                          Final Year Defense Project  \\n                                                          Session 2020 -2024  \\n \\n \\n \\n \\n \\nReport submitted in the partial fulfillment  of the requirements for the degree of B.Sc. \\nElectrical Engineering  \\n \\n \\nSubmitted by:   \\nMr. Syed Umar Raza                             Reg No #20MDELE133  \\nMr. Junaid Badshah                             Reg No #20MDELE159  \\nMr. Samad Latif                             Reg No #20MDELE177  \\n \\nSupervised by: Dr. Jawad Ali  \\n \\n                              Department of Electrical Engineering  \\nUniversity of Engineering & Technology, Mardan  \\n'),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 1}, page_content='2                                                           Declaration  \\n \\nWe hereby declare that this project report/thesis entitled “Loan approval prediction \\nusing machine learning ” submitted to the Electrical Engineering Department, is a \\nrecord of an original work done by us under the guidan ce of Supervisor Dr. Jawad \\nAli and that no part has been plagiarized without citations. Also, this project work is \\nsubmitted in the partial fulfillment of the requirements for the degree of Bachelor of \\nScience  \\n \\n \\n \\n \\n \\n \\nTeam Members  Signature  \\nSyed Umar Raza  _____________________  \\nJunaid Badshah  _____________________  \\nSamad Latif  _____________________  \\n  \\n  \\nSupervisor   \\nDr. Jawad  Ali  \\nSignature:  \\nDate:_________________  \\n '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 2}, page_content='3 \\n FYDP Report Approval  \\nThe Department of Electrical Engineering, University of Engineering & Technology \\nMardan accepts the report in the present form and it satisfies the entire requirements of  \\nthe Final Year Project (FYP).  \\n \\n \\nApproval of FYP Coordinator:  \\nName:   Eng. Salman Sal eem \\n \\nSignature : _______________________  \\n \\n \\nApproval of Chairman:  \\nName:  Prof Dr.  Imran Khan  \\n \\nSignature : _______________________  \\n \\n \\n \\n '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 3}, page_content='4 Plagiarism Check Repor t \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nName of supervisor  \\nDr. Jawad Ali  Signature : '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 4}, page_content=\"5 \\n  \\nContents  \\nLoan -Approval -Prediction using machine learning  ................................ ........................ 1 \\nFinal Year Defense Proje ct ................................ ................................ .............................. 1 \\nSession 2020 -2024  ................................ ................................ ................................ ..1 \\nDeclaration  ................................ ................................ ................................ ...................... 2 \\nFYDP Report Approval  ................................ ................................ ................................ ...3 \\nPlagiarism Check Report  ................................ ................................ ................................ 4 \\nList of Abbreviations  ................................ ................................ ................................ ..... 11 \\n Machine learning  ................................ ................................ ................................ ..........  11 \\nSynthetic minority over -sampling technique  ................................ ................................  11 \\nList of Figures  ................................ ................................ ................................ ...............  11 \\nChapter 1: Introduction  ................................ ................................ ................................ .12 \\n1.1 Background and Motivation  ................................ ................................ ................ 12 \\n1.1.1  Introduction to Loan approval prediction using machine learning  ......... 12 \\n1.2 Problem Statement  ................................ ................................ .............................. 13 \\n1.2.1 Issues with Traditional Loan Approval Methods  ................................ ......... 13 \\n1.2.2 Need for Automation and Data - Driven Decision Making  .......................... 13 \\n1.3 Research Objective  ................................ ................................ ............................. 14 \\n6. Establishment of Performance Metrics: Establish robust metrics to evaluate the \\nmodel's performance, including accuracy, precision, recall, F1 score, and ROC -\\nAUC.  ................................ ................................ ................................ ......................... 14 \\n1.4 Scope of the Study  ................................ ................................ .............................. 14 \\n1.4.1  Data Scope  ................................ ................................ ................................ ...14 \\n1.4.2  Geographical Context  ................................ ................................ ...................... 15 \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 5}, page_content='6 1.4.3  Loan Types Considered  ................................ ................................ ................ 15 \\n1.4.4  Limitations and Assumptions  ................................ ................................ .......15 \\n1.5.1 Impact on Financial Institutions  ................................ ................................ ...15 \\n1.5.2  Benefits to Borrowers  ................................ ................................ .................. 15 \\n1.6   Structure of the Thesis  ................................ ................................ ........................... 16 \\n1.6.1  Overview of Chapters  ................................ ................................ .......................... 16 \\nChapter -2: Literature Review  ................................ ................................ ........................ 16 \\n2.1 Overview  ................................ ................................ ................................ ............. 17 \\n2.1.1 Significance of Literature Review  ................................ ............................... 17 \\n2.2 Traditional Methods of Loan Approval  ................................ ............................... 17 \\n2.2.1 Statistical Methods  ................................ ................................ ....................... 17 \\n2.2.2 Limitations of Traditional Methods  ................................ ............................. 17 \\n2.3 Evolution of  device  gaining knowledge of  in monetary  offerings  ...................... 18 \\n2.3.1  Comparati ve Studies  ................................ ................................ ................... 18 \\n2.4 Popular Machine Learning Models for Loan Approval  ................................ ......18 \\n2.4.1 Decision Trees and Ensemble Methods  ................................ ....................... 18 \\n2.4.2 Boosting Algorithms  ................................ ................................ .................... 18 \\n2.5 Feature Selection and Engineering  ................................ ................................ .....19 \\n2.5.1 Importance of Feature Selection  ................................ ................................ ..19 \\n2.5.2 Impact on Model Accuracy  ................................ ................................ .......... 19 \\n2.6 Handling Imbalanced Data  ................................ ................................ .................. 19 \\n2.6.1 Class Imbalance Issue  ................................ ................................ .................. 19 \\n2.6.2 Techniques for Imbalance Correction  ................................ .......................... 19 \\n2.7 Model Interpretability and Explain ability  ................................ .......................... 20 \\n2.7.1 Importance in Financial Sector  ................................ ................................ ....20 '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 6}, page_content='7 \\n 2.7.2 Techniques for Interpretability  ................................ ................................ .....20 \\n2.8 Real -World Applications and Case Studies  ................................ ......................... 20 \\n2.8.1 Implementation in Financial Institutions  ................................ ..................... 20 \\n2.8.2 Case Study Insights  ................................ ................................ ...................... 20 \\n2.9 Ethical Considerations and Fairness  ................................ ................................ ...21 \\n2.9.1 Bias and Fairness Issues  ................................ ................................ ............... 21 \\n2.9.2 Fairness -Aware Techniques  ................................ ................................ .......... 21 \\n2.10 Gaps in the Literature  ................................ ................................ ........................ 21 \\n2.10.1 Need for Diverse Techniques  ................................ ................................ .....21 \\n2.10.2 Integration of New Data Sources  ................................ ............................... 21 \\nChapter 3: Literature review  ................................ ................................ ......................... 22 \\n3.1 Introduction  ................................ ................................ ................................ ......... 22 \\n3.2 Dataset Selection  ................................ ................................ ................................ .22 \\n3.2.1 Source of Data  ................................ ................................ .............................. 22 \\n3.2.2 Data Description  ................................ ................................ .......................... 23 \\n3.3 Da ta Preprocessing  ................................ ................................ .............................. 27 \\n3.3.1 Data Cleaning  ................................ ................................ ............................... 27 \\n3.3 Data Preprocessing  ................................ ................................ .............................. 27 \\n3.3.1 Data Cleaning  ................................ ................................ ............................... 27 \\n3.3.2 Da ta Transformation  ................................ ................................ ........................ 27 \\n3.3.3 Handling Imbalanced Data  ................................ ................................ ........... 27 \\n3.4 Feature Selection and Engineering  ................................ ................................ .....28 \\n3.4.1 Feature Selection Techniques  ................................ ................................ .......28 \\n3.4.2 Feature Engineering  ................................ ................................ ..................... 28 \\n3.5 Model Selection  ................................ ................................ ................................ ..28 '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 7}, page_content='8 3.5.1 Algorithm Choice  ................................ ................................ ......................... 28 \\n3.5.2 Model Training  ................................ ................................ ............................. 29 \\n3.6 Mode l Evaluation  ................................ ................................ ................................ 29 \\n3.6.1 Evaluation Metrics  ................................ ................................ ....................... 29 \\n3.6.2 Cross -Validation  ................................ ................................ ........................... 29 \\n3.7.1 Software and Tools  ................................ ................................ ....................... 29 \\n3.7.2 Model Deployment  ................................ ................................ ...................... 29 \\n3.8 Conclusion  ................................ ................................ ................................ .......... 30 \\n3.8.1 Summary of Methodology  ................................ ................................ ........... 30 \\n3.8.2 Reflection on Methodological Choices  ................................ ........................ 30 \\nChapter 4 Results and Discussion  ................................ ................................ ................. 30 \\n4.1.1 Overview of Chapter  ................................ ................................ .................... 30 \\n4.1.2 Importance of Results Analysis  ................................ ................................ ....31 \\n4.2 Data Preprocessing Results  ................................ ................................ ................. 31 \\n4.2.1 Data Cleaning  ................................ ................................ ............................... 31 \\n4.2.2 Feature Engineering Outcomes  ................................ ................................ ....33 \\n4.2.3 Data Splitting  ................................ ................................ ............................... 33 \\n4.3 Model Training and Hyper parameter Tuning Results  ................................ ........ 33 \\n4.3.1 Logistic Regression  ................................ ................................ ...................... 33 \\n4.3.2 Decision Trees  ................................ ................................ .............................. 33 \\n4.3.3 Support Vector Machines (SVM)  ................................ ................................ .34 \\n4.3.4 Random Forests  ................................ ................................ ............................ 34 \\n4.3.5 Gr adient Boosting Machines  ................................ ................................ ........ 34 \\n4.4 Model Evaluation  ................................ ................................ ................................ 34 \\n4.4.1 Evaluation Metrics Results  ................................ ................................ .......... 34 '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 8}, page_content='9 \\n 4.4.2 Cross -Validation Performance  ................................ ................................ .....35 \\n4.5 Comparative Analysis of Models  ................................ ................................ ........ 35 \\n4.5.1 Comparison of Performance Metrics  ................................ ........................... 35 \\n4.5.2 Discussion of Best Performing Model  ................................ ......................... 35 \\n4.6 Model Interpretability and Insights  ................................ ................................ .....36 \\n4.6.1 Interpretability Techniques Applied  ................................ ............................. 36 \\n4.6.2 Insights from Model Interpretability  ................................ ............................ 36 \\n4.7 Real -World Testing and Feedback  ................................ ................................ ......36 \\n4.7.1 Pilot Testing Results  ................................ ................................ ..................... 36 \\n4.7.2 Iterative Improvements  ................................ ................................ ................ 37 \\n4.8 Discussion of Results  ................................ ................................ .......................... 37 \\n4.8.1 Implications for Financial Institutions  ................................ ......................... 37 \\n4.8.2 Broader Implications  ................................ ................................ .................... 37 \\nChapter 5: Conclusion  ................................ ................................ ................................ ...38 \\n5.1 Introduction  ................................ ................................ ................................ ......... 38 \\n5.1.1 Overview of Chapter  ................................ ................................ .................... 38 \\n5.1.2 Restating the Research Objectives  ................................ ............................... 38 \\n5.2 Summary of Findings  ................................ ................................ .......................... 38 \\n5.2.1 Data Preprocessing and Feature Engineering  ................................ .................. 38 \\n5.2.2 Model Performance  ................................ ................................ ...................... 38 \\n5.2.3 Model Interpretability  ................................ ................................ .................. 39 \\n5.2.4 Real -World Testing and Feedback  ................................ ............................... 39 \\n5.3 Contributions of the Study  ................................ ................................ .................. 39 \\n5.3.1 Theoretical Contributions ................................ ................................ ............. 39 \\n5.3.2 Practical Contributions  ................................ ................................ ................. 39 '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 9}, page_content='10 5.4 Implications for Financial Institutions  ................................ ................................ 40 \\n5.4.1 Operational Efficiency  ................................ ................................ ................. 40 \\n5.4.2 Risk Management  ................................ ................................ ........................ 40 \\n5.4.3 Customer Experience  ................................ ................................ ................... 40 \\n5.5 Limitations of the Study  ................................ ................................ ...................... 40 \\n5.5.1 Data Limitations  ................................ ................................ ........................... 40 \\n5.5.2 Model Limitations  ................................ ................................ ........................ 41 \\n5.5.3 Generalization to Different Contexts  ................................ ........................... 41 \\n5.6 Future Research Directions  ................................ ................................ ................. 41 \\n5.6.1 Advanced Modeling Techniques  ................................ ................................ ..41 \\n5.6.2 Expanded Feature Sets  ................................ ................................ ................. 41 \\n5.6.3 Real -Time Decision Making  ................................ ................................ ........ 41 \\n5.6.4 Ethical and Fairness Considerations  ................................ ............................ 42 \\n5.7 Conclusion  ................................ ................................ ................................ .......... 42 \\n5.7.1 Recap of Key Points  ................................ ................................ ..................... 42 \\n5.7.2 Final Thoughts  ................................ ................................ ............................. 42 \\nFYDP to CEP Mapping  ................................ ................................ ................................ .42 \\nFYDP to SDGs Mapping  ................................ ................................ .............................. 44 \\nReferences  ................................ ................................ ................................ ..................... 47 \\n   \\n \\n \\n \\n \\n \\n '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 10}, page_content='11 \\n  \\n \\nList of Abbreviations  \\n \\nAbbreviatio n                       Expansion  \\n ML                             Machine learning  \\n SMOTE  synthetic minority over -sampling technique  \\nSHAP                                    Shapley  Additive explanation  \\nLIME                                    Local Interpreta ble Model -agnostic Explanations  \\nRFE Recursive Feature Elimination  \\nSVM  Support Vector Machines  \\n \\nList of Figures  \\nFigure 1 : Machine learning Data description  \\nFigure 2: Graph for number of dependents  and counts  \\nFigure 3 : Flow chart working of project  \\nFigure 4: Result and discussion on result  \\n \\nList of Tables  \\nTable: 3.1  Data selection  \\nTable: 3.2 Data description  \\n \\n \\n \\n \\n \\n \\n '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 11}, page_content=\"12  \\n \\n                                    Chapter 1: Introduction  \\n \\n1.1 Background and Motivation  \\nThe rapid advancements in technology have significantly transformed various \\nindustries, including the financial sector. One critical aspect that has benefited from \\nthese ad vancements is the process of loan approval. Traditionally, loan approval has \\nbeen a manual and subjective process, relying heavily on human judgment and \\nhistorical financial data. However, with the advent of machine learning, there is a \\nshift towards autom ating this process to enhance accuracy, efficiency, and \\nconsistency [ 1]. Machine learning (ML) involves the use of algorithms and statistical \\nmodels to analyze and draw inferences from patterns in data. In the context of loan \\napproval, ML can help in asses sing an applicant's creditworthiness by analyzing a \\nmultitude of factors, such as credit history, income level, employment status, and \\nother financial indicators. This automated approach not only speeds up the decision -\\nmaking process but also reduces the r isk of human error and biases [ 2]. \\n \\n1.1.1 Introduction to Loan approval prediction using machine learning   \\nThe distribution of loans is a central commercial activity for almost every financial \\ninstitution. A primary source of a bank ’s assets comes directly from the profits earned \\nthrough the loans they distribute. The main goal in the banking environment is to \\ninvest their assets safely. Nowadays, many banks and financial institutions approve \\nloans after a rigorous process of verification and validation, yet there is still no \\nguarantee that the selected applicant is the most deserving among all candidates. \\nThrou gh these tools, we can predict whether a specific applicant is reliable, and the \\nentire process of validating features is automated using machine learning techniques. \\nLoan prediction is highly beneficial for both bank employees and applicants. The \\nobjectiv e of this project is to provide a quick, instant, and easy method to select \\ndeserving candidates, offering significant advantages to the bank. The loan prediction \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 12}, page_content='13 \\n system can automatically calculate the weight of each feature involved in loan \\nprocessing, an d apply the same features to new test data according to their related \\nweights. A time limit can be set for the applicant to check whether their loan can be \\nsanctioned. The loan prediction system allows prioritizing certain applications, \\nenabling them to be  processed on a priority basis. This paper is intended exclusively \\nfor the management authority of banks or financial organizations, ensuring that the \\nentire prediction process is conducted privately without any stakeholder interference. \\nResults for specif ic loan identifications can be sent to various branches of banks so \\nthey can take appropriate actions on the applications. This helps other departments \\ncomplete additional formalities efficiently.  \\n1.2 Problem Statement  \\n1.2.1 Issues with Traditional Loan Approval Methods  \\nTraditional loan approval methods are fraught with challenges. The reliance on \\nmanual assessment leads to inconsistencies, as different loan officers may interpret \\nthe same information differently. This subjectivity can result in unfair de cisions, \\nwhere some applicants are approved or rejected based on the personal biases of the \\nloan officer rather than objective criteria . \\nMoreover, manual processes are inefficient and time -consuming. The need to review \\neach application in detail means that  loan approval can take several days or even \\nweeks, leading to frustration among applicants and inefficiencies within the financial \\ninstitution . \\n1.2.2 Need for Automation and Data - Driven Decision Making  \\nThe automation of loan approval processes through ma chine learning addresses these \\nissues by providing a consistent and efficient decision -making framework. Machine \\nlearning models can process vast amounts of data quickly and make decisions based \\non patterns and correlations that may not be apparent to huma n analysts . \\nAutomated, data -driven decision -making ensures that all applicants are assessed \\nusing the same criteria, reducing the likelihood of bias and improving the fairness of '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 13}, page_content=\"14 the process. It also significantly speeds up the loan approval process, enhan cing the \\ncustomer experience and allowing financial institutions to operate more efficiently . \\n \\n1.3 Research Objective  \\n1. The primary objective of this research is to develop and validate a machine \\nlearning model capable of accurately predicting loan approvals . This involves \\nleveraging historical loan application data to train the model and validate its \\nperformance using various metrics.  \\n2. To achieve the primary objective, the following specific objectives are set:  \\n3. Identification of Key Features: Determine the mo st relevant features that impact \\nloan approval decisions, including credit score, income level, and employment \\nstatus.  \\n4. Comparison of Machine Learning Algorithms: Evaluate and compare the \\nperformance of various machine learning algorithms, such as logistic regression, \\ndecision trees, support vector machines, and ensemble methods.  \\n5. Implementation of the Best -Performing Model: Develop a practical \\nimplementation of the best -performing model for use in real -world settings, \\nensuring it is both accurate and efficie nt. \\n6. Establishment of Performance Metrics: Establish robust metrics to evaluate the \\nmodel's performance, including accuracy, precision, recall, F1 score, and ROC -\\nAUC.  \\n1.4 Scope of the Study  \\n   1.4.1  Data Scope  \\nThe study will utilize historical loan application data, which includes financial, \\npersonal, and demographic information of applicants. The data will be sourced from \\nfinancial institutions or publicly available datasets that provide comprehensive details  \\nnecessary for predictive modeling . \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 14}, page_content='15 \\n    4.21.  Geographical Context  \\nWhile the primary focus of the study will be on data from a specific geographical \\nregion or institution, the methodology and findings can be generalized to other \\nregions or institutions with similar characteristics. This ensures t hat the research has  \\nbroader applicability and relevance [ 5]. \\n \\n1.4.3  Loan Types Considered  \\nThe study will consider various types of loans, including personal loans, mortgages, \\nand auto loans. This diversity in loan types will help ensure that the developed model \\nis robust and capable of handling different loan scenarios . \\n1.4.4  Limitations and Assumptions  \\nThe study assumes that the provided data is accurate and representative of the broader \\npopulation. Additionally, it acknowledges the inherent lim itations of machine \\nlearning models, such as the risk of overfitting and the need for large datasets to \\nachieve high accuracy.  \\n1.5.1 Impact on Financial Institutions  \\nAccurate loan approval prediction can significantly impact financial institutions by \\nreduc ing the risk of loan defaults. By leveraging machine learning models, \\ninstitutions can make data -driven decisions that enhance the reliability of their \\nlending processes. This, in turn, can lead to improved financial stability and \\nprofitability . \\n \\n1.5.2  Benefits to Borrowers  \\nBorrowers benefit from a faster and fairer loan approval process. Automated \\ndecision -making reduces the waiting time for loan approvals and ensures that \\ndecisions are based on objective criteria. This transparency builds trust between \\nborrowers and financial institutions, enhancing customer satisfaction and loyalty . \\n \\n1.5.3  Contribution to Academic Research  '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 15}, page_content='16 This study contributes to the academic field by advancing the application of machine \\nlearning in financial services. It provides insights into the most effective machine \\nlearning algorithms for loan approval prediction and highlights the importance of \\nfeature selection and model validation. Additionally, it addresses gaps in the current \\nliterature, including the need for data divers ity and model interpretability.  \\n1.6   Structure of the Thesis  \\n1.6.1  Overview of Chapters  \\nThe thesis is organized into five chapters, each serving a specific purpose:  \\nChapter 1: Introduction  - Provides an overview of the project, including the \\nbackground, problem statement, objectives, scope, and significance.  \\nChapter 2: Literature Review  - Reviews current research on loan approval processes \\nand the application of machine learning in finance.  \\nChapter 3: Project Methodology and Implementation  - Details the r esearch design, \\ndata collection, preprocessing, model training, and validation approaches.  \\nChapter 4: Results and Discussion  - Presents the findings of the study, including \\nmodel performance, key feature analysis, and comparative analysis.  \\nChapter 5: Conclusion and Future Work  - Summarizes the key findings, discusses \\ncontributions to the field, and outlines recommendations for future research.  \\n \\n \\n                              Chapter -2: Literature Review  \\n '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 16}, page_content='17 \\n 2.1 Overview  \\nThe literature review serves as the  foundation for understanding the current state of \\nresearch in the field of loan approval prediction using machine learning. It examines \\nvarious studies, methodologies, and findings related to credit scoring, risk assessment, \\nand the application of machine  learning models in financial services. This chapter \\nprovides a comprehensive review of the existing body of knowledge, identifies gaps, \\nand highlights areas for further research . \\n2.1.1  Significance of Literature Review  \\nUnderstanding the existing literatur e is crucial for identifying best practices, \\nrecognizing limitations in current approaches, and paving the way for innovative \\nsolutions. This review will help contextualize the findings of the current study within \\nthe broader research landscape.  \\n2.2 Tradit ional Methods of Loan Approval  \\n2.2.1 Statistical Methods  \\n \\nLoan approval processes have historically relied on traditional statistical methods and \\nheuristic rules. Early approaches, such as linear regression and logistic regression, have \\nbeen widely used du e to their simplicity and interpretability. These models often use \\ndemographic and financial information to assess creditworthiness.  \\n2.2.2 Limitations of Traditional Methods  \\nDespite their widespread use, traditional methods face limitations, particularly i n \\nhandling non -linear relationships and complex interactions among variables. Studies by \\nHand and Henley (1997) and Thomas (2000) have highlighted these limitations, paving \\nthe way for more advanced techniques [ 6]. '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 17}, page_content='18 2.3 Evolution of  device  gaining knowledge  of in monetary  offerings  \\nThe advent of machine learning has revolutionized financial services, providing more \\nadvanced tools for credit risk evaluation. Machine learning models, such as decision \\ntrees, support vector machines (SVM), and neural networks, o ffer better accuracy and \\nversatility compared to traditional methods.  \\n2.3.1 Comparative  Studies  \\nResearch by Khandani, Kim, and Lo (2010) and Baesens et al. (2005) demonstrated the \\nsuperior performance of these models in predicting loan defaults and improvi ng credit \\nscoring accuracy. These models can capture complex patterns and non -linear \\nrelationships in the data, making them more effective for risk assessment.  \\n \\n2.4 Popular Machine Learning Models for Loan Approval  \\n2.4.1 Decision Trees and Ensemble Methods  \\nSeveral machine learning models have gained popularity in loan approval prediction. \\nDecision trees and ensemble methods, like Random Forests and Gradient Boosting \\nMachines, have been extensively studied and applied. Breiman (2001) introduced \\nRandom Forest s, which aggregate multiple decision trees to enhance predictive \\nperformance [ 7]. \\n2.4.2 Boosting Algorithms  \\nSimilarly, Friedman (2001) proposed Gradient Boosting Machines, which iteratively \\nbuild models to correct errors made by previous models. These ense mble methods have \\nshown high accuracy and robustness in various studies, including those by Bellotti and \\nCrook (2009) and Glassman  et al. (2015).  '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 18}, page_content=\"19 \\n 2.5 Feature Selection and Engineering  \\n2.5.1 Importance of Feature Selection  \\nEfficient feature selection and en gineering are crucial for enhancing model \\nperformance in loan approval prediction. Strategies such as principal component \\nanalysis (PCA), recursive feature elimination (RFE), and genetic algorithms have \\nbeen investigated to identify the most relevant featu res [8].  \\n   \\n2.5.2 Impact on Model Accuracy  \\nStudies by Liu and Schumann (2005) and Luo, Xu, and Liu (2020) have demonstrated \\nthe impact of feature selection on model accuracy. Feature engineering, which involves \\ncreating new variables from existing data, also plays a significant role. This process \\nhelps in uncovering hidden patterns and relationships, enhancing the model's predictive \\npower.  \\n2.6 Handling Imbalanced Data  \\n2.6.1 Class Imbalance Issue  \\nLoan approval datasets often exhibit class imbalance, where the number of approved \\nloans significantly outweighs the number of rejected ones. This imbalance can lead to \\nbiased models that favor the majority class.  \\n2.6.2 Techniques for Imbalance Correction  \\nApproaches such as resampling, Synthetic Minority Over -sampl ing Technique \\n(SMOTE), and cost -sensitive learning have been suggested to address this issue. \\nResearch conducted by He and Ma (2013) and Brown and Mues (2012) has \\ndemonstrated that these techniques can improve model performance on imbalanced \\ndatasets, ensu ring proper representation of minority class instances [9].  \\n \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 19}, page_content='20  \\n2.7 Model Interpretability and Explain ability  \\n2.7.1 Importance in Financial Sector  \\nAs machine learning models become more complex, ensuring their interpretability and \\nexplain ability  becomes cr ucial, especially in the financial sector where transparency is \\nessential.  \\n2.7.2 Techniques for Interpretability  \\nApproaches such as SHAP (Shapley Additive Explanations) and LIME (Local \\nInterpretable Model -agnostic Explanations) have been developed to offer  insights \\ninto model decisions. Research by Lundberg and Lee (2017) and Ribeiro, Singh, and \\nGuestrin (2016) has underscored the importance of these methods in understanding \\nmodel behavior and establishing trust with stakeholders.  \\n2.8 Real -World Application s and Case Studies  \\n2.8.1 Implementation in Financial Institutions  \\nThe application of machine learning models in real -world loan approval scenarios has \\nbeen documented in various case studies. Financial institutions have reported significant \\nimprovements in  accuracy and efficiency by implementing machine learning -based \\nsystems [ 10]. \\n2.8.2 Case Study Insights  \\nCase studies by Akhtar and Bilal (2020) and Twala and Jones (2011) have demonstrated \\nthe practical benefits of these models in different financial conte xts. These studies \\nprovide valuable insights into the challenges and successes of deploying machine \\nlearning models in the financial industry [ 11]. '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 20}, page_content='21 \\n 2.9 Ethical Considerations and Fairness  \\n2.9.1 Bias and Fairness Issues  \\nThe use of machine learning in loan approval raises ethical concerns, particularly \\nregarding fairness and bias. Ensuring that models do not discriminate against certain \\ngroups based on race, gender, or other protected attributes is essential.  \\n2.9.2 Fairness -Aware Techniques  \\nResearch by Baroc as, Hardt, and Narayanan (2019) and Dastin (2018) has emphasized \\nthe importance of fairness -aware machine learning. Techniques such as fairness \\nconstraints, adversarial debasing , and bias detection methods are being developed to \\naddress these concerns. Eth ical considerations must be integrated into the model \\ndevelopment process to ensure fair and equitable treatment of all applicants.  \\n2.10 Gaps in the Literature  \\n2.10.1 Need for Diverse Techniques  \\nDespite significant advancements, there are still gaps in the  literature that need to be \\naddressed. Most studies focus on specific machine learning models without exploring \\nthe full range of available techniques [ 12]. \\n \\n \\n \\n2.10.2 Integration of New Data Sources  \\nThere is also a need for more research on the integration  of alternative data sources, \\nsuch as social media and transaction data, to enhance predictive power. Additionally, '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 21}, page_content=\"22 the impact of dynamic features and real -time decision -making has not been extensively \\nstudied. Addressing these gaps can provide a more comp rehensive understanding of \\nloan approval prediction and improve model performance.  \\n \\n                                  Chapter 3: Literature review  \\n3.1 Introduction  \\nThis chapter details the methodology and implementation steps followed to develop a \\nmachine learning model for loan approval prediction. It covers the dataset selection, \\npreprocessing steps, feature selection and engineering, model selection, evaluation \\nmetrics, and the implementation process. The methodology ensures that the model is \\nrobust, acc urate, and generalizable.  \\n3.2 Dataset Selection   \\n3.2.1 Source of Data  \\nThe dataset utilized for this project was obtained from LendingClub, a peer -to-peer \\nlending company that provides comprehensive loan information. This dataset \\nincludes various features such as loan amount, interest rate, borrower's credit \\nhistory , employment records, and more.  \\nTable: 3.1 Data  selection  \\n \\n\"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 22}, page_content='23 \\n 3.2.2 Data Description  \\nThe dataset consists of several thousand loan applications with features like loan \\npurpose, applicant’s income, credit history, loan amount, and loan status (approved or \\nrejected). The data spans multiple years, providing a rich set of information for analysis.  \\nTable: 3.2 Data  description  \\n                                        Figure 1 : Machine learning Data descr iption  \\n'),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 23}, page_content='24  \\n'),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 24}, page_content='25 \\n   \\n                        Figure 2: Graph for number of dependents  and counts  \\n'),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 25}, page_content='26  \\n \\n \\n \\n'),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 26}, page_content='27 \\n 3.3 Data Preprocessing  \\n3.3.1 Data Cleaning  \\nData cleaning is a crucial step that involves handling missing values, removing \\nduplicates, and correcting inconsistencies. Missing values were imputed using median \\nor mean for numerical features and mode for categorical features. Outliers were \\nidentified and treated to prevent skewing the model results.  \\n3.3 Data Preprocessing  \\n3.3.1 Data Cleaning  \\nData cleaning is a crucial step that involves handling missing values, removing \\nduplicates, and correcting inconsistencies. Missing values were imputed using median \\nor mean for numerical features and mode for categorical features. Outliers were \\nidentified and treated to  prevent skewing the model results.  \\n3.3.2 Data Transformation  \\nData transformation involves normalizing or standardizing numerical features and \\nencoding categorical features. Numerical features were scaled to ensure uniformity, and \\ncategorical features were  converted using techniques such as one -hot encoding or label \\nencoding.  \\n3.3.3 Handling Imbalanced Data  \\nGiven that the dataset is imbalanced with more approved loans than rejected ones, \\ntechniques such as oversampling (SMOTE) and under sampling  were applied  to balance \\nthe class distribution. This step is crucial to ensure that the model does not become \\nbiased towards the majority class.  '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 27}, page_content=\"28 3.4 Feature Selection and Engineering  \\n3.4.1 Feature Selection Techniques  \\nFeature selection was performed using techniques like Recursive Feature Elimination \\n(RFE), which helps in selecting the most significant features by recursively removing \\nless important ones. Other techniques such as feature importance scores from tree -based \\nmodels were also considered [ 13]. \\n3.4.2 Feature Engineering  \\nFeature engineering involved creating new features from existing ones to enhance the \\nmodel's predictive power. For instance, deriving the debt -to-income ratio, length of \\nemployment, and credit u tilization ratios from existing features provided additional \\ninsights into an applicant's creditworthiness.  \\n3.5 Model Selection  \\n3.5.1 Algorithm Choice  \\nVarious machine learning algorithms were considered, including logistic regression, \\ndecision trees, rando m forests, gradient boosting machines (GBM), and neural \\nnetworks. The choice of algorithms was based on balancing interpretability and \\npredictive performance overall.  \\n \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 28}, page_content=\"29 \\n 3.5.2 Model Training  \\nThe selected  fashions  had been  educated  using  cross -validation to  make sure  that \\nthe results  are generalizable and  no longer  over fitted  to the  education  information . \\nHyper parameter  tuning  changed into  carried out  the use of  grid search  and \\nrandom  seek techniques  to optimize  model  performance  \\n3.6 Model Evaluation  \\n3.6.1 Evaluation Metrics  \\nThe models were evaluated using metrics such as accuracy, precision, recall, F1 -\\nscore, and Area Under the Receiver Operating Characteristic Curve (AUC -ROC). \\nThese metrics provide a comprehensive understanding of the model's performance \\n[14] [15].  \\n3.6.2 Cross -Validation  \\nCross -validation techniques like k -fold cross -validation were used to ensure that the \\nmodel's performance is consistent across different subsets of the data. This helps in \\nreducing variance and provides a more reliable esti mate of model performance.  \\n3.7 Implementation  \\n3.7.1 Software and Tools  \\nThe mission  become  implemented  using  Python, leveraging libraries  together \\nwith pandas for  statistics  manipulation, sickest -examine  for version  constructing , \\nand Matplotlib and Seaborne  for facts  visualization. Jupyter Notebooks  have \\nbeen  used for interactive coding and documentation  \\n3.7.2 Model Deployment  \\nThe final model was deployed using Flask, a lightweight web framework for Python. \\nThis allows for the creation of an API that can be used to serve the model predictions to \\nother applications or users. The deployment pipeline included steps for continuous \\nintegration and delivery (CI/CD) to ensure smooth updates and maintenance.  \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 29}, page_content='30 3.8 Conclusion  \\n3.8.1 Summary of Methodology  \\nThis chapter pr ovided a detailed description of the methodology and implementation \\nsteps for developing a machine learning model for loan approval prediction. It covered \\ndataset selection, preprocessing, feature selection and engineering, model selection, \\nevaluation, and  implementation [ 16]. \\n3.8.2 Reflection on Methodological Choices  \\nThe methodological choices made during the project were aimed at building a robust, \\naccurate, and interpretable model. Future work could explore additional preprocessing \\ntechniques, alternative algorithms, and more sophisticated deployment strategies to \\nfurther enhance the model’s performance and usability [ 17]. \\n \\n                             Chapter 4 : Results and Discussion  \\n \\n4.1.1 Overview of Chapter  \\nThis chapter provides a comprehensive evaluation of the results obtained from the loan \\napproval prediction  models developed in this study. It begins with an overview of the \\ndata preprocessing steps and discusses the outcomes of data cleaning, feature \\nengineering, and data splitting. Subsequently, it explores the performance of various \\nmachine learning models, including Logistic Regression, Decision Trees, Support \\nVector Machines (SVM), Random Forests, and Gradient Boosting Machines [18]. The \\ntraining performance of each model, optimal hyperparameters, and evaluation metrics \\nare discussed in detail. The chapter also includes a comparative analysis of these \\nmodels, highlighting their strengths and weaknesses.  '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 30}, page_content='31 \\n Furthermore, it covers the results of real -world testing and user feedback, providing \\ninsights into the practical applicability of the model and areas for fu ture improvement \\n[19] [20] [21].  \\n \\n4.1.2 Importance of Results Analysis  \\nAnalyzing the results is crucial for validating the effectiveness of the machine \\nlearning models and understanding their potential impact on the loan approval \\nprocess. It helps identify  which models perform best in terms of accuracy, precision, \\nrecall, and other evaluation metrics. This analysis also sheds light on the \\ninterpretability of the models, ensuring that the decision -making process is \\ntransparent and understandable to stakehold ers [22]. The findings from this chapter \\nare instrumental in making data -driven decisions for deploying these models in real -\\nworld financial institutions [ 23]. \\n4.2 Data Preprocessing Results  \\n4.2.1 Data Cleaning  \\nThe data cleaning process involved handling missing values and detecting and \\ntreating outliers. Missing values were addressed using median or mode imputation \\ntechniques, ensuring that the dataset was complete and ready for analysis. Outliers \\nwere identifie d through statistical methods and either capped or removed based on \\ntheir impact on the overall data distribution and model performance [ 24]. This step \\nwas crucial to enhance the quality of the data and ensure that the machine learning \\nmodels were trained on accurate and reliable information [ 25]. '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 31}, page_content='32  \\n \\n                                         Figure 3 : Flow chart working of project  \\n \\n \\n                                           Figure 4: Result and discussion on result  \\n \\n   \\n'),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 32}, page_content=\"33 \\n 4.2.2 Feature Engineering Outcomes  \\nFeature engineering played a significant role in enhancing the model's predictive \\ncapabilities. The process involved selecting and transforming features to better \\ncapture the underlying patterns in the data. Key features such as income level, credit \\nscore,  employment status, loan amount, and loan tenure were identified as the most \\nrelevant predictors for loan approval. Additionally, certain features were log -\\ntransformed to normalize their distributions and improve model performance. These \\nengineered feature s provided enhanced inputs for the machine learning algorithms, \\nultimately leading to improved predictive accuracy [26].  \\n4.2.3 Data Splitting  \\nThe dataset was divided into training and testing sets using an 80/20 ratio, ensuring \\nthat the models could be eva luated on unseen data. Cross -validation techniques, \\nincluding k -Fold and Stratified k -Fold, were employed to ensure rigorous evaluation \\nand mitigate overfitting. These methods offered a thorough assessment of the models' \\nperformance by training and validat ing them on multiple subsets of the data. The \\nresults from cross -validation indicated that the models were well -generalized and \\ncapable of consistent performance across different data splits [27].  \\n4.3 Model Training and Hyper parameter  Tuning Results  \\n4.3.1  Logistic Regression  \\nLogistic Regression was one of the baseline models used in this study. The training \\nperformance of Logistic Regression showed consistent accuracy around 85%. The \\nbest hyper parameters  for this model were determined to be a regularizati on \\nparameter (C) of 1.0 and a solver of ' bilinear '. These parameters were optimized using \\ngrid search, which systematically evaluated different combinations to find the most \\neffective configuration.  \\n4.3.2 Decision Trees  \\nDecision Trees demonstrated a high training accuracy of 90%, indicating their \\ncapability to capture complex patterns in the data. The optimal hyper parameters  \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 33}, page_content=\"34 included a maximum depth of 10 and a minimum samples split of 5. These parameters \\nhelped balance the trade -off between model complexity and generalizability, ensuring \\nthat the Decision Tree model performed well on both the training and testing datasets.  \\n4.3.3 Support Vector Machines (SVM)  \\nSupport Vector Machines (SVM) performed well with a training accuracy of 88%. \\nThe best hyper parameters  for SVM included a radial basis function (RBF) kernel \\nwith a C value of 1.0 and a gamma value of 0.1. These parameters were selected \\nthrough an extensiv e hyper parameter  tuning process, which aimed to maximize the \\nmodel's ability to classify loan approvals accurately.  \\n4.3.4 Random Forests  \\nRandom Forests showed robust performance with a training accuracy of 92%. The \\noptimal parameters included 100 trees in  the forest and a maximum depth of 20. \\nRandom Forests benefited from their ensemble nature, which allowed them to \\naverage multiple decision trees and reduce the risk of over fitting , resulting in strong  \\ngeneralization ability.  \\n4.3.5 Gradient Boosting Machi nes \\nGradient Boosting Machines achieved the highest training accuracy of 93%. The best \\nhyper parameters  included 200 boosting stages and a learning rate of 0.1. This model \\nleveraged the boosting technique to sequentially improve the prediction accuracy by \\nfocusing on the errors of previous iterations, making it the most accurate model \\namong those tested.  \\n4.4 Model Evaluation   \\n4.4.1 Evaluation Metrics Results  \\nThe models were evaluated using a variety of metrics to comprehensively assess their \\nperformance. Ac curacy scores ranged from 85% to 93% across different models, with \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 34}, page_content=\"35 \\n Gradient Boosting Machines achieving the highest accuracy. Precision, recall, and F1 \\nscores were also calculated to evaluate the balance between false positives and false \\nnegatives. The ROC -AUC scores provided an overall measure of the models' ability \\nto distinguish between approved and non -approved loans. Confusion matrix analysis \\nfurther detailed the true positives, true negatives, false positives, and false negatives \\nfor each model, offer ing a detailed view of their classification performance.  \\n4.4.2 Cross -Validation Performance  \\nCross -validation strategies, including k -Fold and Stratified k -Fold, were used to ensure \\nthe robustness of the evaluation metrics. k -Fold cross -validation results c onfirmed that \\nthe models' performance was consistent across different subsets of data. Stratified k -\\nFold cross -validation preserved the distribution of the target variable, providing more \\nreliable performance metrics. These validation techniques demonstrat ed that the models \\nwere not overfitting to the training data and were likely to perform well on unseen data.  \\n \\n4.5 Comparative Analysis of Models  \\n4.5.1 Comparison of Performance Metrics  \\nThe comparative analysis revealed that Gradient Boosting Machines had t he highest \\naccuracy and ROC -AUC scores, making it the best performing model. Random \\nForests also showed competitive performance with high accuracy and robustness. \\nLogistic Regression and SVM provided good baseline models with reasonable \\naccuracy and interp retability. The statistical analysis of differences in performance \\nmetrics highlighted the strengths and weaknesses of each model, providing a clear \\nunderstanding of their relative effectiveness.  \\n4.5.2 Discussion of Best Performing Model  \\nThe Gradient Boost ing Machine was chosen as the best -performing model due to its \\nsuperior performance metrics and capability to handle complex data relationships. \\nDespite requiring more computational resources and tuning compared to simpler \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 35}, page_content=\"36 models, its high accuracy and rob ustness made it the most suitable choice for loan \\napproval prediction. This model excelled in focusing on challenging cases and \\niteratively enhancing prediction accuracy. However, its weaknesses included higher \\ncomputational demands and the potential for o verfitting if not carefully tuned  \\n4.6 Model Interpretability and Insights  \\n4.6.1 Interpretability Techniques Applied  \\nTo ensure the transparency and interpretability of the models' decisions, techniques \\nsuch as SHAP (Shapley Additive Explanations) and LIME ( Local Interpretable \\nModel -agnostic Explanations) were utilized. SHAP analysis provided both global and \\nlocal interpretability by explaining the contribution of each feature to the predictions . \\nMeanwhile, LIME offered insights into individual predictions by  highlighting the \\nsignificant factors influencing each decision. These methods were essential for \\nfostering trust in the model's predictions and ensuring transparency in the loan \\napproval process.  \\n4.6.2 Insights from Model Interpretability  \\nThe interpretability analysis revealed several important features that influenced loan \\napproval predictions. Income level and credit score were identified as the most \\ninfluential features, significantly impacting the likelihood of loan approval. \\nEmployment sta tus and loan amount also played significant roles in the decision -\\nmaking process. These insights have important implications for financial institutions, \\nas they highlight the key factors that should be considered when assessing loan \\napplications. Understan ding these features can help institutions refine their loan \\napproval criteria and improve transparency with applicants.  \\n4.7 Real -World Testing and Feedback  \\n4.7.1 Pilot Testing Results  \\nThe models were tested in real -world scenarios through pilot testing, wh ere they were \\napplied to new loan applications to evaluate their practical performance. The real -\"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 36}, page_content=\"37 \\n world performance metrics showed that the models maintained high accuracy and \\nreliability when applied to new data. User feedback from financial institutions a nd \\nloan officers provided valuable insights into the usability and effectiveness of the \\nmodels in actual loan approval processes, highlighting areas for improvement and \\nfurther refinement.  \\n4.7.2 Iterative Improvements  \\nBased on feedback from pilot testing, several iterative improvements were made to \\nenhance the models' performance. Modifications included adjustments to model \\nparameters, data preprocessing techniques, and feature engineering strategies. These \\niterative improvements resulted in enhanced accura cy and user satisfaction, ensuring \\nthat the models were better suited for deployment in real -world financial institutions.  \\n4.8 Discussion of Results  \\n4.8.1 Implications for Financial Institutions  \\nThe findings from this study have significant implications fo r financial institutions. \\nThe use of machine learning models for loan approval prediction can improve \\noperational efficiency by automating the approval process, reducing manual effort, \\nand speeding up decision -making. Enhanced accuracy in predicting loan d efaults can \\nalso improve risk management capabilities, helping institutions minimize losses and \\nmanage their loan portfolios more effectively.  \\n4.8.2 Broader Implications  \\nThe broader implications of this study extend beyond financial institutions to loan \\napplicants and policymakers. For loan applicants, an automated and transparent loan \\napproval process can increase trust and satisfaction. For policymakers, the insights \\ngained from this study can inform . \\n \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 37}, page_content=\"38                                       Chapter 5: Conclusion  \\n5.1 Introduction  \\n5.1.1 Overview of Chapter  \\nIt discusses the limitations encountered during the research and suggests directions for \\nfuture work. The chapter concludes with final thoughts on the overall impact and \\nsignificance of the study.  \\n5.1.2  Restating the Research Objectives  \\nThe main objective of this research was to develop and evaluate machine learning \\nmodels for predicting loan approvals. This involved data preprocessing, feature \\nengineering, model training, hyper parameter  tuning, and performance evaluation. \\nThe goal was to identify the most effective model that financial institutions can use \\nto enhance their loan approval processes.  \\n5.2 Summary of Findings  \\n5.2.1 Data Preprocessing and Feature Engineering  \\nThe data preproces sing phase was crucial in preparing the dataset for model training. \\nTechniques such as handling missing values, outlier detection, and feature \\nengineering were employed to improve the dataset's quality. Key features such as \\nincome level, credit score, empl oyment status, loan amount, and loan tenure were \\nidentified as significant predictors of loan approval [28][29]  \\n5.2.2 Model Performance  \\nMultiple machine learning models were trained and evaluated, including Logistic \\nRegression, Decision Trees, Support Vect or Machines (SVM), Random Forests, and \\nGradient Boosting Machines. The Gradient Boosting Machine emerged as the best -\\nperforming model, achieving the highest accuracy and demonstrating robust \\nperformance across various evaluation metrics [30].  \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 38}, page_content=\"39 \\n 5.2.3 Model I nterpretability  \\nInterpretability techniques like SHAP and LIME were employed to understand the \\nmodels' decision -making processes. These techniques identified key features \\ninfluencing loan approvals, providing transparency and insights into the models' \\npredictions. Income level and credit score were the most influential factors in predicting \\nloan approvals.  \\n5.2.4 Real -World Testing and Feedback  \\nThe models were tested in real -world scenarios, and feedback from financial institutions \\nindicated high accuracy an d reliability. Iterative improvements based on this feedback \\nfurther enhanced the models' performance, ensuring they were well -suited for practical \\ndeployment [ 31] [32] . \\n5.3 Contributions of the Study  \\n5.3.1 Theoretical Contributions  \\nThis study contributes to the theoretical understanding of machine learning applications \\nin financial decision -making. It provides a comprehensive analysis of various models' \\nperformance and highlights the importance of feature selection and model \\ninterpretability in predictive analytics [ 33]. \\n5.3.2 Practical Contributions  \\nPractically, this research offers a robust machine learning framework for loan approval \\nprediction that financial institutions can implement to enhance their decision -making \\nprocesses. The insights from model i nterpretability also help institutions understand the \\nkey factors influencing loan approvals, improving transparency and trust with \\napplicants [34] [35][36] . \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 39}, page_content='40 5.4 Implications for Financial Institutions  \\n5.4.1 Operational Efficiency  \\nThe implementation of mach ine learning models can significantly improve operational \\nefficiency in financial institutions by automating the loan approval process. This \\nreduces manual effort, speeds up decision -making, and ensures consistent and accurate \\nassessments.  \\n5.4.2 Risk Manag ement  \\nEnhanced predictive accuracy in loan approvals helps financial institutions manage \\nrisks better by accurately identifying potential defaults. This improves the overall health \\nof the loan portfolio and minimizes financial losses [ 37]. \\n5.4.3 Customer E xperience  \\nAutomated and transparent loan approval processes can enhance customer experience \\nby providing quicker decisions and clear explanations for approvals or rejections. This \\nbuilds trust and satisfaction among loan applicants.  \\n5.5 Limitations of the Study  \\n5.5.1 Data Limitations  \\nThe study faced limitations related to the availability and quality of data. Some datasets \\nhad missing values or were not comprehensive enough to capture all relevant aspects of \\nloan applications. Additionally, the study relied  on historical data, which may not fully \\nrepresent future trends [ 38]. '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 40}, page_content=\"41 \\n 5.5.2 Model Limitations  \\nWhile the Gradient Boosting Machine performed well, it required significant \\ncomputational resources and fine -tuning. The study also did not explore more advanced  \\nmodels or techniques that could potentially improve performance further.  \\n5.5.3 Generalization to Different Contexts  \\nThe models developed in this study were based on specific datasets and may not \\ngeneralize well to different contexts or financial instituti ons with varying loan approval \\ncriteria. Further testing and adaptation may be needed for broader applicability [ 38]. \\n5.6 Future Research Directions  \\n5.6.1 Advanced Modeling Techniques  \\nFuture research could explore more advanced machine learning techniques such as deep \\nlearning or ensemble methods that combine multiple models to improve predictive \\naccuracy and robustness.  \\n5.6.2 Expanded Feature Sets  \\nIncorporating additional features such as social media activity, spending patterns, or \\nalternative credit scor es could enhance the models' predictive power. Future studies \\nshould also focus on the dynamic nature of these features over time [ 39]. \\n5.6.3 Real -Time Decision Making  \\nImplementing real -time data processing and decision -making systems could further \\nimprove  the efficiency and responsiveness of loan approval processes. Research should \\nexplore the integration of streaming data and real -time analytics.  \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 41}, page_content=\"42 5.6.4 Ethical and Fairness Considerations  \\nFuture work should address ethical considerations and ensure fairnes s in the models' \\npredictions. This includes identifying and mitigating biases in the data and ensuring that \\nthe models do not unfairly discriminate against any group of applicants.  \\n5.7 Conclusion  \\n5.7.1 Recap of Key Points  \\nThis study developed and evaluated  machine learning models for loan approval \\nprediction, with the Gradient Boosting Machine emerging as the best performing model. \\nKey features influencing loan approvals were identified, and real -world testing \\nconfirmed the models' practical applicability. The research highlighted the importance \\nof model interpretability and provided valuable insights for financial institutions.  \\n5.7.2 Final Thoughts  \\nThe findings of this study demonstrate the potential of machine learning to transform \\nloan approval processes in financial institutions. By improving efficiency, accuracy, and \\ntransparency, these models can enhance both operational performance and customer \\nsatisfaction. Future research should continue to explore advanced techniques and \\nbroader applications to full y realize the benefits of predictive analytics in financial \\ndecision -making [ 40]. \\nFYDP to CEP Mapping  \\n \\nS \\n#  Attribute  Complex Problem  Targeted  \\nJustification  \\n   (Yes/ \\nNo)   \"),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 42}, page_content='43 \\n 1. Preamble  *Engineering problems \\nwhich cannot be resolved \\nwithout in -depth \\nengineering knowledge.  \\nAnd have some or all of \\nthe characteristics listed \\nbelow:    \\n2. Range of \\nconflicting \\nrequirements  Involve wide -ranging or \\nconflicting technical, \\nengineering and other \\nissues.    \\n3. Depth of \\nAnalysis required  Have no obvious solution \\nand require abstract \\nthinking, originality in \\nanalysis to formulate \\nsuitable models.    \\n4. Depth of \\nKnowledge \\nrequired  Requires research -based \\nknowledge much of which \\nis at, or informed by, the \\nforefront of the professional \\ndiscipline and which allows \\na fundamentals -based, first \\nprinciples analytical \\napproach.    \\n5. Familiarity of \\nissues  Involve infrequently \\nencountered issues    \\n6. Extent of \\napplication codes  Are outside problems \\nencompassed by standards   '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 43}, page_content='44 and codes of practice for \\nprofessional engineering.  \\n7. Extent of \\nstakeholder \\ninvolved  Involve diverse groups of \\nstakeholders with widely \\nvarying needs.    \\n8. Consequences  Have significant \\nconsequences in a range of \\ncontexts.    \\n9. interdependence  \\ne  Are high level problems \\nincluding many component \\nparts or sub problems?     \\nFYDP to SDGs Mapping  \\n \\nSustainable Development Goals (SDGs)   \\nGoal#  Description  Justification  \\n \\nGoal 1  NO POVERTY  \\nEnd poverty in all its forms everywhere   \\nGoal 2  ZERO HUNGER  \\nEnd hunger, achieve food security and \\nimproved nutrition and promote sustainable \\nagriculture   \\nGoal 3  GOOD HEALTH & WELL -BEING  \\nEnsure healthy lives and promote well -being for \\nall at all ages   \\nGoal 4  QUALITY EDUCATION   '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 44}, page_content='45 \\n Ensure inclusive and equitable quality \\neducation and promote lifelong learning \\nopportunities for all  \\nGoal 5  GENDER EQUALITY  \\nAchieve gender equality and empower all \\nwomen and girls   \\nGoal 6  CLEAN WATER AND SANITATION  \\nEnsure availability and sustainable \\nmanagement of water and sanitation for all   \\nGoal 7  AFFORDABLE AND CLEAN ENERGY  \\nEnsure access to affordable, reliable, \\nsustainable, and modern energy for all   \\nGoal 8  DECENT WORK AND ECONOMIC \\nGROWTH  \\nPromote sustained , inclusive, and sustainable \\neconomic growth, full and productive \\nemployment, and decent work for all   \\nGoal 9  INDUSTRY , INNOV ATION, AND  \\nINFRASTRUCTURE  \\nBuild resilient infrastructure, promote inclusive \\nand sustainable industrialization and foster \\ninnovation.   \\nGoal \\n10  REDUCED INEQUALITIES  \\nReduce inequality within and among countries   \\nGoal \\n11  SUSTAINABLE CITIES AND \\nCOMMUNITIES  \\nMake cities and human settlements inclusive, \\nsafe, resilient and sustainable   '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 45}, page_content='46 Goal \\n12  RESPONSIBLE CONSUMPTION AND  \\nPRODUCTION  \\nEnsure sustainable consumption and production \\npatterns   \\nGoal \\n13  CLIMATE ACTION  \\nTake urgent action to combat climate change \\nand its impacts   \\nGoal \\n14  LIFE BELOW WATER  \\nConserve and sustainably use the oceans, sea \\nand marine resources for sustainable \\ndevelopment   \\nGoal \\n15  LIFE ON LAND  \\nProtect, restore and promote sustainable use of \\nterrestrial ecosystems, sustainably manage \\nforests, combat desertification, and halt and \\nreverse land degradation and halt biodiversity \\nloss   \\nGoal \\n16  PEACE, JUSTICE AND STRONG \\nINSTITUTIONS  \\nPromote peaceful  and inclusive societies for \\nsustainable development, provide access to \\njustice for all and build   \\n effective,  \\naccountable and inclusive institutions at all \\nlevels   \\nGoal \\n17  PARTNERSHIPS  \\nStrengthen the means of implementation and \\nrevitalize the Global Partnership for Sustainable \\nDevelopment   '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 46}, page_content='47 \\n References  \\n[1] O. I. Abiodun, A. Jantan, A. E. Omolara , K. V. Dada, N. A. Mohamed, and H. \\nArshad, \"State -of-the-art in artificial neural network applications: A survey,\" Heliyon , \\nvol. 4, no. 11, p. e00938, Nov. 2018.  \\n[2] P. M. Addo, D. Guegan, and B. Hassani, \"Credit risk analysis using machine and \\ndeep learn ing models,\" Risks , vol. 6, no. 2, p. 38, Jun. 2018.  \\n[3] S. Agarwal and V. Rathod, \"Comparative study on credit scoring models,\" Int. J. \\nRecent Technol. Eng. , vol. 8, no. 2, pp. 3370 -3377, 2019.  \\n[4] N. Akhtar and M. Bilal, \"Analyzing the effect of feature selection on the \\nperformance of machine learning algorithms for credit risk prediction,\" J. Comput. \\nTheor. Nanosci. , vol. 17, no. 5 -6, pp. 2567 -2573, May -Jun. 2020.  \\n[5] F. A. S. Al -Shamery and R. J. Yousif, \"Loan approval prediction using machine \\nlearning and neural network techniques,\" Int. J. Comput. Sci. Network Secur. , vol. 19, \\nno. 8, pp. 120 -127, Aug. 2019.  \\n[6] H. A. Alaka, L. O. Oyedele, H. A. Owolabi, and S. O. Ajayi, \"Systematic review \\nof bankruptcy prediction models: Towards a framework for tool se lection,\" Expert \\nSyst. Appl. , vol. 94, pp. 164 -184, Oct. 2018.  \\n[7] S. Ali, M. Malik, and S. Majeed, \"A comparative analysis of machine learning \\nmodels for credit risk prediction,\" J. Inf. Commun. Technol. , vol. 15, no. 2, pp. 231 -\\n243, 2021.  \\n[8] B. Baesens,  T. Van Gestel, M. Stepanova, J. Suykens, and J. Vanthienen, \"Neural \\nnetwork survival analysis for personal loan data,\" J. Oper. Res. Soc. , vol. 56, no. 9, pp. \\n1089 -1098, Sep. 2005.  '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 47}, page_content='48 [9] T. Bellotti and J. Crook, \"Support vector machines for credit scoring and discovery \\nof significant features,\" Expert Syst. Appl. , vol. 36, no. 2, pp. 3302 -3308, Mar. 2009.  \\n[10] L. Breiman, \"Random forests,\" Mach. Learn. , vol. 45, no. 1, pp. 5 -32, Oct. 2001.  \\n[11] I. Brown and C. Mues, \"An experimental comparison of classification algorithms \\nfor imbalanced credit scoring data sets,\" Expert Syst. Appl. , vol. 39, no. 3, pp. 3446 -\\n3453, Mar. 2012.  \\n[12] R. Caruana and A. Niculescu -Mizil, \"An empirical comparis on of supervised \\nlearning algorithms,\" in Proc. 23rd Int. Conf. Mach. Learn. , 2006, pp. 161 -168. \\n[13] T. Chen and C. Guestrin, \"XGBoost: A scalable tree boosting system,\" in Proc. \\n22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining , 2016, pp. 785 -794. \\n[14] T. G. Dietterich, \"Ensemble methods in machine learning,\" in Int. Workshop \\nMulti. Classifier Syst. , 2000, pp. 1 -15. \\n[15] M. Doumpos and J. R. Figueira, \"Feature selection in machine learning: \\nImproving interpretability of supervised models using featu re importance techniques,\" \\nEur. J. Oper. Res. , vol. 274, no. 3, pp. 1048 -1054, Dec. 2019.  \\n[16] B. Efron and T. Hastie, Computer Age Statistical Inference , vol. 5. Cambridge \\nUniv. Press, 2016.  \\n[17] J. H. Friedman, \"Greedy function approximation: A gradient boosting machine,\" \\nAnn. Stat. , pp. 1189 -1232, Dec. 2001.  \\n[18] S. Ghosh and D. L. Reilly, \"Credit card fraud detection with a neural -network,\" in \\nProc. 27th Hawaii Int. Conf. Syst. Sci. , vol. 3, pp. 621 -630, 1994.  '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 48}, page_content='49 \\n [19] J. M. Gutiérrez, S. Lozano, and J. I. Moreno, \"Machine learning for credit risk: A \\ncomparison of logistic regression, decision trees and neural networks,\" Appl. Sci. , vol. \\n11, no. 8, p. 3610, 2021.  \\n[20] D. J. Hand and W. E. Henley, \"Statistical classification methods in consumer \\ncredit scoring : a review,\" J. R. Stat. Soc. A Stat. Soc. , vol. 160, no. 3, pp. 523 -541, \\n1997.  \\n[21] H. He and Y. Ma, Eds., Imbalanced Learning: Foundations, Algorithms, and \\nApplications . John Wiley & Sons, 2013.  \\n[22] C. L. Huang, M. C. Chen, and C. J. Wang, \"Credit scori ng with a data mining \\napproach based on support vector machines,\" Expert Syst. Appl. , vol. 33, no. 4, pp. \\n847-856, Nov. 2007.  \\n[23] A. E. Khandani, A. J. Kim, and A. W. Lo, \"Consumer credit -risk models via \\nmachine -learning algorithms,\" J. Bank. Finance , vol. 34, no. 11, pp. 2767 -2787, Nov. \\n2010.  \\n[24] Y. Kim and J. H. Ahn, \"A corporate credit rating model using multi -class support \\nvector machines with an ordinal pairwise partitioning approach,\" Comput. Oper. Res. , \\nvol. 39, no. 8, pp. 1800 -1811, Aug. 2012.  \\n[25] S. B. Kotsiantis, \"Decision trees: A recent overview,\" Artif. Intell. Rev. , vol. 39, \\nno. 4, pp. 261 -283, Jun. 2013.  \\n[26] S. Lessmann, B. Baesens, H. V. Seow, and L. C. Thomas, \"Benchmarking state -\\nof-the-art classification algorithms for credit scoring: A  ten-year update,\" Eur. J. \\nOper. Res. , vol. 247, no. 1, pp. 124 -136, Nov. 2015.  \\n[27] Y. Liu and M. Schumann, \"Data mining feature selection for credit scoring \\nmodels,\" J. Oper. Res. Soc. , vol. 56, no. 9, pp. 1099 -1108, Sep. 2005.  '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 49}, page_content='50 [28] W. Luo, H. Xu, and B.  Liu, \"A machine learning approach for credit scoring: \\nIntegrating feature selection and model optimization,\" Neural Comput. Appl. , vol. 32, \\nno. 14, pp. 10865 -10875, Jul. 2020.  \\n[29] R. Malhotra and D. K. Malhotra, \"Evaluating consumer loans using neural \\nnetworks,\" Omega , vol. 31, no. 2, pp. 83 -96, Apr. 2003.  \\n[30] E. W. Ngai, Y. Hu, Y. H. Wong, Y. Chen, and X. Sun, \"The application of data \\nmining techniques in financial fraud detection: A classification framework and an \\nacademic review of literature,\" Decis.  Support Syst. , vol. 50, no. 3, pp. 559 -569, Feb. \\n2011.  \\n[31] S. Oreski, G. Oreski, and D. Oreski, \"Hybrid system with genetic algorithm and \\nartificial neural networks and its application to retail credit risk assessment,\" Expert \\nSyst. Appl. , vol. 39, no. 1 6, pp. 12605 -12609, Nov. 2012.  \\n[32] G. A. Paleologo, A. Elisseeff, and G. Antonini, \"Subagging for credit scoring \\nmodels,\" Eur. J. Oper. Res. , vol. 201, no. 2, pp. 490 -499, Jun. 2010.  \\n[33] H. T. Pham and E. Triantaphyllou, \"The impact of overfitting and \\novergeneralization on the classification accuracy in data mining,\" in Artif. Intell. Appl. , \\npp. 195 -202, 2008.  \\n[34] F. Provost and T. Fawcett, \"Robust classification for imprecise environments,\" \\nMach. Learn. , vol. 42, no. 3, pp. 203 -231, Mar. 2001.  \\n[35] S. R. Sain and M. J. Sain, \"Evaluating the performance of machine learning \\nalgorithms for credit risk prediction,\" in 2017 IEEE 11th Int. Conf. Appl. Inf. \\nCommun. Technol. (AICT) , 2017, pp. 1 -5. \\n[36] L. C. Thomas, \"A survey of credit and behavioural s coring: Forecasting financial \\nrisk of lending to consumers,\" Int. J. Forecast. , vol. 16, no. 2, pp. 149 -172, Jun. 2000.  '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 50}, page_content='51 \\n [37] C. F. Tsai and M. L. Chen, \"Credit rating by hybrid machine learning \\ntechniques,\" Appl. Soft Comput. , vol. 10, no. 2, pp. 374 -380, Mar. 2010.  \\n[38] B. Twala and M. C. Jones, \"Improving automated loan approval with machine \\nlearning,\" Intell. Syst. Account. Finance Manage. , vol. 18, no. 1, pp. 1 -16, 2011.  \\n[39] G. Wang, J. Ma, L. Huang, and K. Xu, \"Two credit scoring models based on dual \\nstrategy ensemble trees,\" Knowl. -Based Syst. , vol. 26, pp. 61 -68, Feb. 2012.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 51}, page_content='52 Coding:  \\n 1. Data collection  \\n1. loan_id  \\n2. no_of_dependents  \\n3. education  \\n4. self_employed>>  \\n5. income_annum  \\n6 . loan_amount  \\n7. loan_term  \\n8 . cibil_score  \\n9. residential_assets_value  \\n10. commercial_assets_value  \\n11. luxury_assets_value  \\n12. bank_asset_value  \\n13. loan_status  \\n12 are dependent veriable 1 is independent veraible  \\n \\n     2. Importing dependencies  \\n1 import numpy as np  \\n1. import pandas as pd  \\n2. import  seaborn as sns  \\n3. import matplotlib.pyplot as plt  \\n4. from sklearn import svm  \\n5. from sklearn.ensemble import RandomForestClassifier  \\n6. from sklearn.linear_model import LogisticRegression  \\n7. from sklearn.tree import DecisionTreeClassifier  \\n8. from sklearn.model_selecti on import train_test_split, GridSearchCV  \\n9. from sklearn.metrics import accuracy_score, f1_score  '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 52}, page_content='53 \\n 10. from sklearn.model_selection import cross_val_score  \\n11. from sklearn.compose import ColumnTransformer  \\n12. OneHotEncoder,StandardScaler,LabelEncoder  \\n13. import warnings  \\n14. warnings.filterwarnings(\"ignore\")  \\n3. Pre processing  \\nkeyboard_arrow_down LogisticRegression  \\n1 model2 = LogisticRegression()  \\n▾ LogisticRegression  \\nLogisticRegression()  \\n1 model2.fit(X_train, Y_train)  \\n1 print(f\"Training Score:{model2.score(X_train,Y_train) *100}%\")  \\nTraining Score:92.03513909224012%  \\n1 print(f\"Test score:{model2.score(X_test,Y_test)*100}%\")  \\nTest score:89.92974238875878%  \\nkeyboard_arrow_down Decision tree  \\n▾  \\nDecisionTreeClassifier  \\nDecisionTreeClassifier(ccp_alpha=0.01)   \\nmodel3 = DecisionT reeClassifier(ccp_alpha=0.01)  \\nmodel3.fit(X_train, Y_train)  \\n1 y_pred = model3.predict(X_train)  \\nfrom sklearn.metrics import confusion_matrix,recall_score,precision_score,f1_score  \\nprint(\"confusion metrix: \\\\n \", confusion_matrix(Y_train, y_pred))  \\nprint( \"precision score: \\\\n \", precision_score(Y_train, y_pred))  \\nprint(\"recall score: \\\\n \", recall_score(Y_train, y_pred))  \\nprint(\"f1 score: \\\\n \", f1_score(Y_train, y_pred))  \\nconfusion metrix:  '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 53}, page_content='54 [[2120 0]  \\n[ 146 1149]]  \\nprecision score:  \\n1.0  \\nrecall score:  \\n0.887 2586872586873  \\nf1 score:  \\n0.9402618657937807  \\n11/12 1 print(f\"Training Score:{model3.score(X_train,Y_train)*100}%\")  \\nTraining Score:95.72474377745242%  \\n1 print(f\"Test score:{model3.score(X_test,Y_test)*100}%\")  \\nTest score:95.43325526932084%  \\nkeyboard_arrow _down support vector machine(SVM)  \\n1 model4 = clf = svm.SVC(kernel=\\'linear\\')  \\n▾  \\nSVC  \\nSVC(kernel=\\'linear\\')  \\n1 model4.fit(X_train, Y_train)  \\n1 print(f\"Training Score:{model4.score(X_train,Y_train)*100}%\")  \\nTraining Score:92.79648609077599%  \\n1 print(f\"Test  score:{model4.score(X_test,Y_test)*100}%\")  \\nTest score:91.68618266978923%  \\nkeyboard_arrow_down User Input  \\n#You can use this code for user input  \\ndef predict_loan_approval():  \\nno_of_dependents = int(input(\"Enter the number of dependents: \"))  \\neducation = input(\"Enter education (0 for Graduate and 1 for Not Graduate): \")  \\nself_employed = input(\"Are you self -employed? (1 for Yes and 0 for No): \")  '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 54}, page_content='55 \\n 4. Data processing  \\nX=dataset.drop(\\' loan_status\\',axis=1)  \\nY=dataset[\\' loan_status\\']  \\nX_train,X_test, Y_train,Y_te st = train_test_split(X,Y,test_size=0.2,random_state=42)  \\ntrans ormer = ColumnTransformer(transformers=[  \\n(\\'tnf1\\', OneHotEncoder(sparse=False, drop=\\'first\\'),[\\' education\\',\\' self_employed\\']  \\n(\\'tnf2\\', StandardScaler(),[\\' no_of_dependents\\',\\' income_annum\\',\\' lo an_amount\\',\\' lo  \\n\\' commercial_assets_value\\',\\' luxury_assets_value\\',\\' ban  \\n],remainder=\\'passthrough\\')  \\n1 X_train = transformer.fit_transform(X_train)  \\n1 X_test = transformer.fit_transform(X_test)  \\nlabel_encoder = LabelEncoder()  \\nY_train = label_encoder.fit_ transform(Y_train)  \\nY_test = label_encoder.fit_transform(Y_test)  \\n5.  Deep learning  \\n1. #You can use this code for user input  \\n2. def predict_loan_approval():  \\n3. no_of_dependents = int(input(\"Enter the number of dependents: \"))  \\n4. education = input(\"Enter education (0 for Graduate and 1 for Not Graduate): \")  \\n5. self_employed = input(\"Are you self -employed? (1 for Yes and 0 for No): \")  \\n6. income_annum = float(input(\"Enter the annual income: \"))  \\n7. loan_amount = float(input(\"Enter the loan amount: \"))  \\n8. loan_term = int(input(\"Enter the loan term (in months): \"))  \\n9. cibil_score = int(input(\"Enter the CIBIL score: \"))  \\n10. residential_assets_value = float(input(\"Enter the residential assets value: \"))  \\n11. commercial_assets_value = float(input(\"Enter the  commercial assets value: \"))  \\n12. luxury_assets_value = float(input(\"Enter the luxury assets value: \"))  \\n13. bank_asset_value = float(input(\"Enter the bank asset value:\"))  '),\n Document(metadata={'source': '/kaggle/input/thesis/Thesis.pdf', 'page': 55}, page_content='56 14. user_data = pd.DataFrame(  \\n15. [[no_of_dependents, education, self_employed, income_annum, loa n_amount, \\nloan_term, cibil_score,  \\n16. residential_assets_value, commercial_assets_value, luxury_assets_value, \\nbank_asset_value]],  \\n17. columns=[\\'no_of_dependents\\', \\'education\\', \\'self_employed\\',  \\n18. \\'income_annum\\', \\'loan_amount\\', \\'loan_term\\', \\'cibil_score\\',  \\n19. \\'residen tial_assets_value\\', \\'commercial_assets_value\\',  \\n20. \\'luxury_assets_value\\', \\'bank_asset_value\\',])  \\n21. # # Make prediction  \\n22. prediction = model.predict(user_data.to_numpy())[0]  \\n23. print(f\" \\\\nLoan Approval Prediction:(0 for rejected and 1 for approved) \\n{prediction}\")  \\n24. predict_loan_approval()  \\n \\n \\n \\n ')]"},"metadata":{}}]},{"cell_type":"markdown","source":"## split documents into chunking","metadata":{}},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:43:30.209870Z","iopub.execute_input":"2024-09-07T05:43:30.210166Z","iopub.status.idle":"2024-09-07T05:43:30.220962Z","shell.execute_reply.started":"2024-09-07T05:43:30.210135Z","shell.execute_reply":"2024-09-07T05:43:30.220166Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## embeded documents","metadata":{}},{"cell_type":"code","source":"model_name = \"BAAI/bge-small-en\"\nmodel_kwargs = {\"device\": \"cpu\"}\nencode_kwargs = {\"normalize_embeddings\": True}\nhf_embeddings = HuggingFaceBgeEmbeddings(\n    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n)\nvectorstore = FAISS.from_documents(documents=splits, \n                                    embedding=hf_embeddings)\n\nretriever = vectorstore.as_retriever()","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:43:30.222206Z","iopub.execute_input":"2024-09-07T05:43:30.222618Z","iopub.status.idle":"2024-09-07T05:44:11.563368Z","shell.execute_reply.started":"2024-09-07T05:43:30.222576Z","shell.execute_reply":"2024-09-07T05:44:11.562388Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aab09e2f9c964fab99b4ce3339d87e0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfdd722b20ed40fb8db5feba13535a56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/90.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3808f0c13be4a58b272f6b151c9a2b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde4e1024fca45abb7f017cc1bf0a39e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9ca04b6efd544c89d3ac7ac26a4e3cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee9f1c4f114a43febf69f5c624791535"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fcbb031e62c45dd85af46aa63f21b6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"530f83553b0744f8a8750d010cfd50d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38fdf15ab8204078978a4450978ceb46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ee8b7d15b5473ba692ee67248ef3eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcc1bfe868ce4ed78874a82b85003232"}},"metadata":{}}]},{"cell_type":"code","source":"# Prompt\nprompt = hub.pull(\"rlm/rag-prompt\")","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:44:11.564687Z","iopub.execute_input":"2024-09-07T05:44:11.565410Z","iopub.status.idle":"2024-09-07T05:44:11.813582Z","shell.execute_reply.started":"2024-09-07T05:44:11.565348Z","shell.execute_reply":"2024-09-07T05:44:11.812657Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langsmith/client.py:5515: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n  prompt = loads(json.dumps(prompt_object.manifest))\n","output_type":"stream"}]},{"cell_type":"code","source":"# LLM I use llama3 of 8b parameter\nllm = ChatGroq(model=\"llama3-8b-8192\", temperature=0)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:44:11.814863Z","iopub.execute_input":"2024-09-07T05:44:11.815266Z","iopub.status.idle":"2024-09-07T05:44:11.967673Z","shell.execute_reply.started":"2024-09-07T05:44:11.815222Z","shell.execute_reply":"2024-09-07T05:44:11.966694Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:44:11.970933Z","iopub.execute_input":"2024-09-07T05:44:11.971610Z","iopub.status.idle":"2024-09-07T05:44:11.975893Z","shell.execute_reply.started":"2024-09-07T05:44:11.971566Z","shell.execute_reply":"2024-09-07T05:44:11.974921Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Chain\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:44:11.977155Z","iopub.execute_input":"2024-09-07T05:44:11.977530Z","iopub.status.idle":"2024-09-07T05:44:11.988385Z","shell.execute_reply.started":"2024-09-07T05:44:11.977488Z","shell.execute_reply":"2024-09-07T05:44:11.987557Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Question from your documents\nprint(rag_chain.invoke(\"who is the supervisor of  junaid badshah?\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:44:11.989460Z","iopub.execute_input":"2024-09-07T05:44:11.989814Z","iopub.status.idle":"2024-09-07T05:44:12.332048Z","shell.execute_reply.started":"2024-09-07T05:44:11.989771Z","shell.execute_reply":"2024-09-07T05:44:12.331135Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Dr. Jawad Ali is the supervisor of Junaid Badshah.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(rag_chain.invoke(\"enlist me the project mate member\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-07T05:44:12.333217Z","iopub.execute_input":"2024-09-07T05:44:12.333544Z","iopub.status.idle":"2024-09-07T05:44:12.590298Z","shell.execute_reply.started":"2024-09-07T05:44:12.333508Z","shell.execute_reply":"2024-09-07T05:44:12.589339Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Here are the project team members:\n\n* Syed Umar Raza\n* Junaid Badshah\n* Samad Latif\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}