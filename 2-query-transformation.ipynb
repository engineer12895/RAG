{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9279380,"sourceType":"datasetVersion","datasetId":5616354}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qU langchain_community pypdf\n!pip install langchain-groq\n!pip install sentence_transformers\n!pip install faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:50:46.162610Z","iopub.execute_input":"2024-09-08T19:50:46.163094Z","iopub.status.idle":"2024-09-08T19:52:03.225786Z","shell.execute_reply.started":"2024-09-08T19:50:46.163036Z","shell.execute_reply":"2024-09-08T19:52:03.224343Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n#os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n#os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\nos.environ['LANGCHAIN_PROJECT'] = 'advanced-rag'\nos.environ['LANGCHAIN_API_KEY'] = \"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\"\nos.environ['GROQ_API_KEY'] = \"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\"","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:52:03.228593Z","iopub.execute_input":"2024-09-08T19:52:03.229127Z","iopub.status.idle":"2024-09-08T19:52:03.236877Z","shell.execute_reply.started":"2024-09-08T19:52:03.229070Z","shell.execute_reply":"2024-09-08T19:52:03.235233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bs4\nfrom langchain import hub\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_groq import ChatGroq\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.load import dumps, loads","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:52:03.238532Z","iopub.execute_input":"2024-09-08T19:52:03.238936Z","iopub.status.idle":"2024-09-08T19:52:04.761720Z","shell.execute_reply.started":"2024-09-08T19:52:03.238896Z","shell.execute_reply":"2024-09-08T19:52:04.760563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loader = PyPDFLoader(\"/kaggle/input/thesis/Thesis.pdf\",)\ndocs = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:52:04.764763Z","iopub.execute_input":"2024-09-08T19:52:04.765922Z","iopub.status.idle":"2024-09-08T19:52:07.348914Z","shell.execute_reply.started":"2024-09-08T19:52:04.765861Z","shell.execute_reply":"2024-09-08T19:52:07.347766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:52:07.350292Z","iopub.execute_input":"2024-09-08T19:52:07.350665Z","iopub.status.idle":"2024-09-08T19:52:07.365104Z","shell.execute_reply.started":"2024-09-08T19:52:07.350627Z","shell.execute_reply":"2024-09-08T19:52:07.363620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Index\nmodel_name = \"BAAI/bge-small-en\"\nmodel_kwargs = {\"device\": \"cpu\"}\nencode_kwargs = {\"normalize_embeddings\": True}\nhf_embeddings = HuggingFaceBgeEmbeddings(\n    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n)\nvectorstore = FAISS.from_documents(documents=splits, \n                                    embedding=hf_embeddings)\n\nretriever = vectorstore.as_retriever()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:52:07.366603Z","iopub.execute_input":"2024-09-08T19:52:07.367317Z","iopub.status.idle":"2024-09-08T19:53:07.880501Z","shell.execute_reply.started":"2024-09-08T19:52:07.367273Z","shell.execute_reply":"2024-09-08T19:53:07.879140Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multi Query Prompt","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate\n\n# Multi Query: Different Perspectives\ntemplate = \"\"\"You are an AI language model assistant. Your task is to generate five \ndifferent versions of the given user question to retrieve relevant documents from a vector \ndatabase. By generating multiple perspectives on the user question, your goal is to help\nthe user overcome some of the limitations of the distance-based similarity search. \nProvide these alternative questions separated by newlines. Original question: {question}\"\"\"\nprompt_perspectives = ChatPromptTemplate.from_template(template)\n\ngenerate_queries = (\n    prompt_perspectives \n    | ChatGroq(temperature=0) \n    | StrOutputParser() \n    | (lambda x: x.split(\"\\n\"))\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:07.882302Z","iopub.execute_input":"2024-09-08T19:53:07.883425Z","iopub.status.idle":"2024-09-08T19:53:08.077387Z","shell.execute_reply.started":"2024-09-08T19:53:07.883339Z","shell.execute_reply":"2024-09-08T19:53:08.076324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_queries.invoke(\"Who is DR Jawad ALI and Junaid Badshah\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:08.078923Z","iopub.execute_input":"2024-09-08T19:53:08.079298Z","iopub.status.idle":"2024-09-08T19:53:08.832886Z","shell.execute_reply.started":"2024-09-08T19:53:08.079258Z","shell.execute_reply":"2024-09-08T19:53:08.831720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.load import dumps, loads\n\ndef get_unique_union(documents: list[list]):\n    \"\"\" Unique union of retrieved docs \"\"\"\n    # Flatten list of lists, and convert each Document to string\n    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n    # Get unique documents\n    unique_docs = list(set(flattened_docs))\n    # Return\n    return [loads(doc) for doc in unique_docs]\n\n# Retrieve\nquestion = \"Who is DR Jawad ALI and Junaid Badshah\"\nretrieval_chain = generate_queries | retriever.map() | get_unique_union\ndocs = retrieval_chain.invoke({\"question\":question})\nlen(docs)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:08.834342Z","iopub.execute_input":"2024-09-08T19:53:08.834750Z","iopub.status.idle":"2024-09-08T19:53:09.574362Z","shell.execute_reply.started":"2024-09-08T19:53:08.834710Z","shell.execute_reply":"2024-09-08T19:53:09.573190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from operator import itemgetter\nfrom langchain_core.runnables import RunnablePassthrough\n\n# RAG\ntemplate = \"\"\"Answer the following question based on this context:\n\n{context}\n\nQuestion: {question}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\nllm = ChatGroq(temperature=0)\n\nfinal_rag_chain = (\n    {\"context\": retrieval_chain, \n     \"question\": itemgetter(\"question\")} \n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nfinal_rag_chain.invoke({\"question\":question})","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:09.578707Z","iopub.execute_input":"2024-09-08T19:53:09.579075Z","iopub.status.idle":"2024-09-08T19:53:10.556545Z","shell.execute_reply.started":"2024-09-08T19:53:09.579038Z","shell.execute_reply":"2024-09-08T19:53:10.555381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RAG Fusion query","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate\n\n# RAG-Fusion: Related\ntemplate = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\nGenerate multiple search queries related to: {question} \\n\nOutput (4 queries):\"\"\"\nprompt_rag_fusion = ChatPromptTemplate.from_template(template)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:10.558007Z","iopub.execute_input":"2024-09-08T19:53:10.558435Z","iopub.status.idle":"2024-09-08T19:53:10.564364Z","shell.execute_reply.started":"2024-09-08T19:53:10.558380Z","shell.execute_reply":"2024-09-08T19:53:10.563266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_core.output_parsers import StrOutputParser\nfrom langchain_groq import ChatGroq\n\ngenerate_queries = (\n    prompt_rag_fusion \n    | ChatGroq(temperature=0)\n    | StrOutputParser() \n    | (lambda x: x.split(\"\\n\"))\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:10.565973Z","iopub.execute_input":"2024-09-08T19:53:10.566463Z","iopub.status.idle":"2024-09-08T19:53:10.610881Z","shell.execute_reply.started":"2024-09-08T19:53:10.566389Z","shell.execute_reply":"2024-09-08T19:53:10.609669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.load import dumps, loads\n\ndef reciprocal_rank_fusion(results: list[list], k=60):\n    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n        and an optional parameter k used in the RRF formula \"\"\"\n    fused_scores = {}\n    for docs in results:\n        for rank, doc in enumerate(docs):\n            doc_str = dumps(doc)\n            if doc_str not in fused_scores:\n                fused_scores[doc_str] = 0\n            previous_score = fused_scores[doc_str]\n            fused_scores[doc_str] += 1 / (rank + k)\n    reranked_results = [\n        (loads(doc), score)\n        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n    ]\n    return reranked_results\n\nretrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\ndocs = retrieval_chain_rag_fusion.invoke({\"question\": question})\nlen(docs)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:10.612360Z","iopub.execute_input":"2024-09-08T19:53:10.612812Z","iopub.status.idle":"2024-09-08T19:53:11.086446Z","shell.execute_reply.started":"2024-09-08T19:53:10.612770Z","shell.execute_reply":"2024-09-08T19:53:11.085302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_core.runnables import RunnablePassthrough\n\n# RAG\ntemplate = \"\"\"Answer the following question based on this context:\n\n{context}\n\nQuestion: {question}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\nfinal_rag_chain = (\n    {\"context\": retrieval_chain_rag_fusion, \n     \"question\": itemgetter(\"question\")} \n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nfinal_rag_chain.invoke({\"question\":question})","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:11.088099Z","iopub.execute_input":"2024-09-08T19:53:11.088602Z","iopub.status.idle":"2024-09-08T19:53:11.930562Z","shell.execute_reply.started":"2024-09-08T19:53:11.088549Z","shell.execute_reply":"2024-09-08T19:53:11.929328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decomposition","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate\n\n# Decomposition\ntemplate = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\nThe goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\nGenerate multiple search queries related to: {question} \\n\nOutput (3 queries):\"\"\"\nprompt_decomposition = ChatPromptTemplate.from_template(template)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:11.932096Z","iopub.execute_input":"2024-09-08T19:53:11.932580Z","iopub.status.idle":"2024-09-08T19:53:11.941919Z","shell.execute_reply.started":"2024-09-08T19:53:11.932536Z","shell.execute_reply":"2024-09-08T19:53:11.940497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_groq import ChatGroq\nfrom langchain_core.output_parsers import StrOutputParser\n\n# LLM\nllm = ChatGroq(temperature=0)\n\n# Chain\ngenerate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n\n# Run\nquestion = \"Who is DR Jawad ALI and Junaid Badshah\"\nquestions = generate_queries_decomposition.invoke({\"question\":question})","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:11.943663Z","iopub.execute_input":"2024-09-08T19:53:11.944799Z","iopub.status.idle":"2024-09-08T19:53:12.239074Z","shell.execute_reply.started":"2024-09-08T19:53:11.944750Z","shell.execute_reply":"2024-09-08T19:53:12.237645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:12.240550Z","iopub.execute_input":"2024-09-08T19:53:12.240950Z","iopub.status.idle":"2024-09-08T19:53:12.248456Z","shell.execute_reply.started":"2024-09-08T19:53:12.240909Z","shell.execute_reply":"2024-09-08T19:53:12.247214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prompt\ntemplate = \"\"\"Here is the question you need to answer:\n\n\\n --- \\n {question} \\n --- \\n\n\nHere is any available background question + answer pairs:\n\n\\n --- \\n {q_a_pairs} \\n --- \\n\n\nHere is additional context relevant to the question: \n\n\\n --- \\n {context} \\n --- \\n\n\nUse the above context and any background question + answer pairs to answer the question: \\n {question}\n\"\"\"\n\ndecomposition_prompt = ChatPromptTemplate.from_template(template)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:12.249900Z","iopub.execute_input":"2024-09-08T19:53:12.250325Z","iopub.status.idle":"2024-09-08T19:53:12.258796Z","shell.execute_reply.started":"2024-09-08T19:53:12.250286Z","shell.execute_reply":"2024-09-08T19:53:12.257727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from operator import itemgetter\nfrom langchain_core.output_parsers import StrOutputParser\n\ndef format_qa_pair(question, answer):\n    \"\"\"Format Q and A pair\"\"\"\n    \n    formatted_string = \"\"\n    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n    return formatted_string.strip()\n\n# llm\nllm = ChatGroq(temperature=0)\n\nq_a_pairs = \"\"\nfor q in questions:\n    \n    rag_chain = (\n    {\"context\": itemgetter(\"question\") | retriever, \n     \"question\": itemgetter(\"question\"),\n     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n    | decomposition_prompt\n    | llm\n    | StrOutputParser())\n\n    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n    q_a_pair = format_qa_pair(q,answer)\n    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:12.260149Z","iopub.execute_input":"2024-09-08T19:53:12.260568Z","iopub.status.idle":"2024-09-08T19:53:31.651148Z","shell.execute_reply.started":"2024-09-08T19:53:12.260526Z","shell.execute_reply":"2024-09-08T19:53:31.649844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:31.652823Z","iopub.execute_input":"2024-09-08T19:53:31.653324Z","iopub.status.idle":"2024-09-08T19:53:31.661662Z","shell.execute_reply.started":"2024-09-08T19:53:31.653261Z","shell.execute_reply":"2024-09-08T19:53:31.660318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Answer individually \n","metadata":{}},{"cell_type":"code","source":"# Answer each sub-question individually \n\nfrom langchain import hub\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough, RunnableLambda\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_groq import ChatGroq\n\n# RAG prompt\nprompt_rag = hub.pull(\"rlm/rag-prompt\")\n\ndef retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n    \"\"\"RAG on each sub-question\"\"\"\n    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n    rag_results = []\n    \n    for sub_question in sub_questions:\n        retrieved_docs = retriever.get_relevant_documents(sub_question)\n        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n                                                                \"question\": sub_question})\n        rag_results.append(answer)\n    return rag_results,sub_questions\nanswers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:53:31.663362Z","iopub.execute_input":"2024-09-08T19:53:31.663861Z","iopub.status.idle":"2024-09-08T19:54:05.436797Z","shell.execute_reply.started":"2024-09-08T19:53:31.663808Z","shell.execute_reply":"2024-09-08T19:54:05.435779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_qa_pairs(questions, answers):\n    \"\"\"Format Q and A pairs\"\"\"\n    \n    formatted_string = \"\"\n    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n    return formatted_string.strip()\n\ncontext = format_qa_pairs(questions, answers)\n\n# Prompt\ntemplate = \"\"\"Here is a set of Q+A pairs:\n\n{context}\n\nUse these to synthesize an answer to the question: {question}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\nfinal_rag_chain = (\n    prompt\n    | llm\n    | StrOutputParser()\n)\n\nfinal_rag_chain.invoke({\"context\":context,\"question\":question})","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:54:05.438339Z","iopub.execute_input":"2024-09-08T19:54:05.438756Z","iopub.status.idle":"2024-09-08T19:54:09.902700Z","shell.execute_reply.started":"2024-09-08T19:54:05.438715Z","shell.execute_reply":"2024-09-08T19:54:09.901584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Response prompt \nresponse_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n\n# {normal_context}\n# {step_back_context}\n\n# Original Question: {question}\n# Answer:\"\"\"\nresponse_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n\nchain = (\n    {\n        # Retrieve context using the normal question\n        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n        # Retrieve context using the step-back question\n        \"step_back_context\": generate_queries_step_back | retriever,\n        # Pass on the question\n        \"question\": lambda x: x[\"question\"],\n    }\n    | response_prompt\n    | ChatGroq(temperature=0)\n    | StrOutputParser()\n)\n\nchain.invoke({\"question\": question})","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:54:13.683438Z","iopub.execute_input":"2024-09-08T19:54:13.683815Z","iopub.status.idle":"2024-09-08T19:54:31.701908Z","shell.execute_reply.started":"2024-09-08T19:54:13.683777Z","shell.execute_reply":"2024-09-08T19:54:31.700652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HyDE\n","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate\n\n# HyDE document genration\ntemplate = \"\"\"Please write a scientific paper passage to answer the question\nQuestion: {question}\nPassage:\"\"\"\nprompt_hyde = ChatPromptTemplate.from_template(template)\n\nfrom langchain_core.output_parsers import StrOutputParser\n\ngenerate_docs_for_retrieval = (\n    prompt_hyde | ChatGroq(temperature=0) | StrOutputParser() \n)\n\n# Run\nquestion = \"Who is DR Jawad ALI and Junaid Badshah\"\ngenerate_docs_for_retrieval.invoke({\"question\":question})","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:57:05.282652Z","iopub.execute_input":"2024-09-08T19:57:05.283130Z","iopub.status.idle":"2024-09-08T19:57:05.953625Z","shell.execute_reply.started":"2024-09-08T19:57:05.283078Z","shell.execute_reply":"2024-09-08T19:57:05.952471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrieve\nretrieval_chain = generate_docs_for_retrieval | retriever \nretireved_docs = retrieval_chain.invoke({\"question\":question})\nretireved_docs","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:57:13.219504Z","iopub.execute_input":"2024-09-08T19:57:13.219959Z","iopub.status.idle":"2024-09-08T19:57:14.049045Z","shell.execute_reply.started":"2024-09-08T19:57:13.219915Z","shell.execute_reply":"2024-09-08T19:57:14.047838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RAG\ntemplate = \"\"\"Answer the following question based on this context:\n\n{context}\n\nQuestion: {question}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\nfinal_rag_chain = (\n    prompt\n    | llm\n    | StrOutputParser()\n)\n\nfinal_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:54:41.564067Z","iopub.execute_input":"2024-09-08T19:54:41.564537Z","iopub.status.idle":"2024-09-08T19:54:58.015066Z","shell.execute_reply.started":"2024-09-08T19:54:41.564492Z","shell.execute_reply":"2024-09-08T19:54:58.013856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}